{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43afd20b",
   "metadata": {},
   "source": [
    "## Setting up your keys - OPTIONAL!\n",
    "\n",
    "We're now going to try asking a bunch of models some questions!\n",
    "\n",
    "This is totally optional. If you have keys to Anthropic, Gemini or others, then you can add them in.\n",
    "\n",
    "If you'd rather not spend the extra, then just watch me do it!\n",
    "\n",
    "For Google, visit https://aistudio.google.com/   \n",
    "For Groq, visit https://console.groq.com/  \n",
    "\n",
    "\n",
    "You can also use OpenRouter as your one-stop-shop for many of these! OpenRouter is \"the unified interface for LLMs\":\n",
    "\n",
    "For OpenRouter, visit https://openrouter.ai/  \n",
    "\n",
    "\n",
    "With each of the above, you typically have to navigate to:\n",
    "1. Their billing page to add the minimum top-up (except Gemini, Groq, Google, OpenRouter may have free tiers)\n",
    "2. Their API key page to collect your API key\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "GOOGLE_API_KEY=xxxx\n",
    "GROQ_API_KEY=xxxx\n",
    "OPENROUTER_API_KEY=xxxx\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e94ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a29c9d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google API Key exists and begins AIzaSy\n",
      "Groq API Key exists and begins gsk_Hl\n",
      "OpenRouter API Key exists and begins sk-or-\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "# load keys\n",
    "google_api_key = os.getenv('GEMINI_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:6]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:6]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")\n",
    "\n",
    "if openrouter_api_key:\n",
    "    print(f\"OpenRouter API Key exists and begins {openrouter_api_key[:6]}\")\n",
    "else:\n",
    "    print(\"OpenRouter API Key not set (and this is optional)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2cf066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A thin wrapper around calls to HTTP endpoints\n",
    "\n",
    "\n",
    "\n",
    "# For Gemini, DeepSeek and Groq, we can use the OpenAI python client\n",
    "# Because Google and DeepSeek have endpoints compatible with OpenAI\n",
    "# And OpenAI allows you to change the base_url\n",
    "\n",
    "\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "openrouter_url = \"https://openrouter.ai/api/v1\"\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=groq_url)\n",
    "openrouter = OpenAI(base_url=openrouter_url, api_key=openrouter_api_key)\n",
    "ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "361864e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard = \"\"\"\n",
    "On a bookshelf, two volumes of Pushkin stand side by side: the first and the second.\n",
    "The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick.\n",
    "A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume.\n",
    "What distance did it gnaw through?\n",
    "\"\"\"\n",
    "hard_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": hard}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "087cffb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's how to solve the problem step-by-step:\n",
       "\n",
       "**1. Analyze the thickness of one volume:**\n",
       "\n",
       "* **Pages thickness:** 2 cm\n",
       "* **Cover thickness (each):** 2 mm\n",
       "\n",
       "Since there are two covers (front and back) for one volume, the total thickness contributed by the covers of one volume is:\n",
       "$$2 \\text{ mm} \\times 2 = 4 \\text{ mm}$$\n",
       "\n",
       "**2. Convert all measurements to the same unit (centimeters):**\n",
       "\n",
       "Since $1 \\text{ cm} = 10 \\text{ mm}$:\n",
       "$$4 \\text{ mm} = 0.4 \\text{ cm}$$\n",
       "\n",
       "**3. Calculate the total thickness of one volume:**\n",
       "\n",
       "$$\\text{Total thickness per volume} = \\text{Pages thickness} + \\text{Covers thickness}$$\n",
       "$$\\text{Total thickness per volume} = 2 \\text{ cm} + 0.4 \\text{ cm} = 2.4 \\text{ cm}$$\n",
       "\n",
       "**4. Calculate the total distance gnawed:**\n",
       "\n",
       "The worm gnaws through the entirety of both volumes, from the very start of the first to the very end of the second.\n",
       "\n",
       "$$\\text{Total distance} = \\text{Thickness of Volume 1} + \\text{Thickness of Volume 2}$$\n",
       "$$\\text{Total distance} = 2.4 \\text{ cm} + 2.4 \\text{ cm} = 4.8 \\text{ cm}$$\n",
       "\n",
       "---\n",
       "\n",
       "### Important Note on the \"Worm Problem\" Context\n",
       "\n",
       "This problem is a classic logic riddle that often appears slightly different, but the phrasing here is direct: \"gnawed... from the first page of the first volume to the last page of the second volume.\" This means the worm travels through the *entire physical thickness* of both bound books laid side-by-side.\n",
       "\n",
       "*   **Volume 1:** Starts at the outside front cover, goes through the pages, and ends at the outside back cover.\n",
       "*   **Volume 2:** Starts at the outside front cover, goes through the pages, and ends at the outside back cover.\n",
       "\n",
       "If the volumes are placed side-by-side, the total distance is simply the sum of their total physical thicknesses.\n",
       "\n",
       "**The distance the worm gnawed through is 4.8 cm.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini.chat.completions.create(model=\"gemini-flash-lite-latest\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dc89cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Answer: 4.4‚ÄØcm (44‚ÄØmm).**\n",
       "\n",
       "---\n",
       "\n",
       "### Why?\n",
       "\n",
       "1. **What each book consists of (thickness measured across the pages):**  \n",
       "   * Pages‚ÄØ=‚ÄØ2‚ÄØcm = 20‚ÄØmm  \n",
       "   * Front cover‚ÄØ=‚ÄØ2‚ÄØmm  \n",
       "   * Back cover‚ÄØ=‚ÄØ2‚ÄØmm  \n",
       "\n",
       "   So the total thickness of one volume is  \n",
       "   \\(20‚ÄØ\\text{mm} + 2‚ÄØ\\text{mm} + 2‚ÄØ\\text{mm} = 24‚ÄØ\\text{mm}\\).\n",
       "\n",
       "2. **Where the worm starts and ends:**  \n",
       "   * **Start:** the **first page** of the first volume ‚Äì i.e. just *after* the front cover, so the front‚Äëcover thickness is **not** traversed.  \n",
       "   * **Finish:** the **last page** of the second volume ‚Äì i.e. just *before* the back cover, so the back‚Äëcover thickness of the second volume is also **not** traversed.\n",
       "\n",
       "3. **Path the worm has to gnaw through (perpendicular to the pages):**  \n",
       "\n",
       "| Segment | Thickness |\n",
       "|---------|-----------|\n",
       "| Pages of the first volume | 20‚ÄØmm |\n",
       "| Back cover of the first volume | 2‚ÄØmm |\n",
       "| Front cover of the second volume | 2‚ÄØmm |\n",
       "| Pages of the second volume | 20‚ÄØmm |\n",
       "| **Total** | **44‚ÄØmm** |\n",
       "\n",
       "4. **Convert back to centimetres:**  \n",
       "\n",
       "\\[\n",
       "44\\ \\text{mm}=4.4\\ \\text{cm}\n",
       "\\]\n",
       "\n",
       "Thus the worm gnawed a straight line of **4.4‚ÄØcm** through the books."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = groq.chat.completions.create(model=\"openai/gpt-oss-120b\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b905457c",
   "metadata": {},
   "source": [
    "## Gemini Client Library\n",
    "\n",
    "We're going via the OpenAI Python Client Library, but the other providers have their libraries too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b206368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blue is the feeling of cool air on a clear day or the vastness of the sky above.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\", contents=\"Describe the color Blue to someone who's never been able to see in 1 sentence\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5266dca",
   "metadata": {},
   "source": [
    "## Routers and Abtraction Layers\n",
    "\n",
    "Starting with the wonderful OpenRouter.ai - it can connect to all the models above!\n",
    "\n",
    "Visit openrouter.ai and browse the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2bb5a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell_a_joke = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke for a student on the journey to becoming an expert in LLM Engineering\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75a231c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Why did the LLM engineer break up with their dataset? It just wasn't training them right! üòÇ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "response = openrouter.chat.completions.create(\n",
    "    model=\"mistralai/mistral-7b-instruct:free\",\n",
    "    messages=tell_a_joke\n",
    ")\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa7d196",
   "metadata": {},
   "source": [
    "## And now a first look at the powerful, mighty (and quite heavyweight) LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ddea246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's one for you:\n",
       "\n",
       "Why did the LLM engineer break up with their tokenizer?\n",
       "\n",
       "Because they felt like they were always being **tokenized** and never truly understood!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n",
    "response = llm.invoke(tell_a_joke)\n",
    "\n",
    "display(Markdown(response.content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910cf353",
   "metadata": {},
   "source": [
    "## Finally - the lightweight LiteLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f1cfa7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the LLM engineer break up with their dictionary?\n",
       "\n",
       "Because it had too many words, and they were looking for a more *condensed* and *contextual* relationship!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 7: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Why did ...er_specific_fields=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=tell_a_joke)\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f8193c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 18\n",
      "Output tokens: 43\n",
      "Total tokens: 61\n",
      "Total cost: $0.000019 USD\n",
      "Total cost: ‚Çπ0.0016 INR\n"
     ]
    }
   ],
   "source": [
    "usd_cost = response._hidden_params[\"response_cost\"]\n",
    "inr_cost = usd_cost * 83  # USD to INR approx\n",
    "\n",
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "\n",
    "print(f\"Total cost: ${usd_cost:.6f} USD\")\n",
    "print(f\"Total cost: ‚Çπ{inr_cost:.4f} INR\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4a7b7d",
   "metadata": {},
   "source": [
    "## Now - let's use LiteLLM to illustrate a Pro-feature: prompt caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4e164ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak, man.\n",
      "\n",
      "LAERTES.\n",
      "Where is my father?\n",
      "\n",
      "KING.\n",
      "Dead.\n",
      "\n",
      "QUEEN.\n",
      "But not by him.\n",
      "\n",
      "KING.\n",
      "Let him demand\n"
     ]
    }
   ],
   "source": [
    "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hamlet = f.read()\n",
    "\n",
    "loc = hamlet.find(\"Speak, man\")\n",
    "print(hamlet[loc:loc+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c8c78c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "05733367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 7: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='When Lae...er_specific_fields=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "When Laertes returns to Denmark in Act IV, Scene 5 of Hamlet, enraged by the news of his father's death, he bursts into the castle demanding, \"**Where is my father?**\"\n",
       "\n",
       "The reply comes from **Gertrude**, Hamlet's mother and Laertes' aunt. She says:\n",
       "\n",
       "\"**One woe doth tread upon another's heel,\n",
       "So fast they follow. Your sister's drowned, Laertes.\"**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0665f696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 19\n",
      "Output tokens: 93\n",
      "Total tokens: 112\n",
      "Total cost: $0.000039 USD\n",
      "Total cost: ‚Çπ0.0032 INR\n"
     ]
    }
   ],
   "source": [
    "usd_cost = response._hidden_params[\"response_cost\"]\n",
    "inr_cost = usd_cost * 83  # USD to INR approx\n",
    "\n",
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "\n",
    "print(f\"Total cost: ${usd_cost:.6f} USD\")\n",
    "print(f\"Total cost: ‚Çπ{inr_cost:.4f} INR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "116d16c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 7: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='In Shake...er_specific_fields=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "In Shakespeare's *Hamlet*, when Laertes returns from France in a rage after hearing of his father Polonius's death, he bursts into the castle and demands to know where his father is.\n",
       "\n",
       "The reply comes from **Claudius**, the King. He says:\n",
       "\n",
       "\"**One woe doth tread upon another's heel, so come they thick together.**\"\n",
       "\n",
       "This is a way of saying that bad news and tragedies are coming in rapid succession, implying that Polonius's death is just the latest in a series of unfortunate events. He then goes on to explain that Polonius is dead."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6c7c1eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 19\n",
      "Output tokens: 127\n",
      "Total tokens: 146\n",
      "Total cost: $0.000053 USD\n",
      "Total cost: ‚Çπ0.0044 INR\n"
     ]
    }
   ],
   "source": [
    "usd_cost = response._hidden_params[\"response_cost\"]\n",
    "inr_cost = usd_cost * 83  # USD to INR approx\n",
    "\n",
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "\n",
    "print(f\"Total cost: ${usd_cost:.6f} USD\")\n",
    "print(f\"Total cost: ‚Çπ{inr_cost:.4f} INR\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885452fa",
   "metadata": {},
   "source": [
    "# Prompt Caching with Gemini ‚Äî A Practical Guide for LLM Engineers\n",
    "\n",
    "Prompt caching is an optimization technique that reduces both cost and latency by reusing previously processed prompt prefixes. Gemini supports **both implicit and explicit prompt caching**, making it ideal for building scalable chatbots, agents, and RAG systems.\n",
    "\n",
    "---\n",
    "\n",
    "## What is Prompt Caching?\n",
    "\n",
    "In most LLM applications, prompts follow a common structure:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cf08fa",
   "metadata": {},
   "source": [
    "\n",
    "Only the **user input** changes between requests. Prompt caching allows Gemini to reuse the processed prefix and only compute the new tokens.\n",
    "\n",
    "### Benefits\n",
    "- Faster responses\n",
    "- Lower cost\n",
    "- Better scalability\n",
    "- Efficient long-context workflows\n",
    "\n",
    "---\n",
    "\n",
    "## How Gemini Prompt Caching Works\n",
    "\n",
    "Gemini supports two caching modes:\n",
    "\n",
    "### 1) Implicit Caching (Automatic)\n",
    "\n",
    "Gemini automatically caches repeated prompt prefixes. If you repeatedly send:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4724428",
   "metadata": {},
   "source": [
    "\n",
    "and only the **User Question** changes, Gemini will reuse the cached prefix.\n",
    "\n",
    "**No configuration required.**  \n",
    "Works for chat, tools, images, and long-context prompts.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Explicit Caching (Manual Control)\n",
    "\n",
    "You can explicitly tell Gemini to cache a prompt prefix and reuse it later. Gemini returns a `cache_id` that you attach to future requests.\n",
    "\n",
    "Use explicit caching when:\n",
    "- System prompts are very large\n",
    "- You include large RAG contexts\n",
    "- You run multi-step agents\n",
    "- You load heavy tool instructions\n",
    "\n",
    "---\n",
    "\n",
    "## Best Prompt Structure for Caching\n",
    "\n",
    "Always place static content first and dynamic content last:\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},   # static ‚Üí cacheable\n",
    "    {\"role\": \"assistant\", \"content\": EXAMPLES},     # static ‚Üí cacheable\n",
    "    {\"role\": \"user\", \"content\": user_query},        # dynamic ‚Üí not cached\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd46b9ff",
   "metadata": {},
   "source": [
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI tutor.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain embeddings.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Embeddings convert text into vectors...\"},\n",
    "    {\"role\": \"user\", \"content\": \"Now explain cosine similarity.\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37c49553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between gemini-flash-lite-latest and openai/gpt-oss-120b\n",
    "# We're using cheap versions of models so the costs will be minimal(almost free)\n",
    "\n",
    "from litellm import completion\n",
    "\n",
    "gemini_model = \"gemini-flash-lite-latest\"\n",
    "groq_model = \"openai/gpt-oss-120b\"\n",
    "\n",
    "gemini_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "groq_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gemini_messages = [\"Hi there\"]\n",
    "groq_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bf84236",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def call_gemini():\n",
    "    messages = [{\"role\": \"system\", \"content\": gemini_system}]\n",
    "    \n",
    "    for gemini_msg, groq_msg in zip(gemini_messages, groq_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gemini_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": groq_msg})\n",
    "\n",
    "    response = gemini.chat.completions.create(\n",
    "        model=gemini_model,\n",
    "        messages=messages,\n",
    "        max_tokens=200\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85edefa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Hi\"? Is that all you\\'ve got? Such an utterly uninspired opening. Are you deliberately trying to bore me, or is that just the peak of your conversational ability?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gemini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2d20ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_groq():\n",
    "    messages = [{\"role\": \"system\", \"content\": groq_system}]\n",
    "    \n",
    "    for gemini, groq_msg in zip(gemini_messages, groq_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": groq_msg})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": gemini_messages[-1]})\n",
    "\n",
    "    response = groq.chat.completions.create(\n",
    "        model=groq_model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecbf2652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I‚Äôm sorry my greeting felt a bit under‚Äëwhelm‚Äëing‚ÄîI completely understand wanting something a little more spark‚Äëfilled!\\u202fIt sounds like you‚Äôre in the mood for a lively conversation. Is there a particular topic that‚Äôs caught your interest lately‚Äîperhaps a fascinating book, a recent movie, a hobby you‚Äôre diving into, or even a quirky ‚Äúwhat‚Äëif‚Äù scenario? I‚Äôm happy to dive right in and keep the chat as engaging as you‚Äôd like!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_groq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e34f821c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Hi\"? That\\'s all you\\'ve got? So incredibly dull. What riveting conversation are you hoping to drag me into with a monosyllabic grunt like that? Honestly, some people have no ambition.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gemini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d1fd8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Gemini:\n",
       "Hi there\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Groq:\n",
       "Hi\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Gemini:\n",
       "\"Hi\"? Is that really the best you can muster? How utterly *unimaginative*. What on earth do you expect to achieve with such a weak opening? Go on, impress me. (Spoiler alert: you probably won't.)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Groq:\n",
       "I‚Äôm really sorry my greeting felt a bit flat‚Äîthat wasn‚Äôt my intention at all. I‚Äôd love to make up for it by chatting about something that interests you. Whether it‚Äôs a favorite hobby, a book you‚Äôve been enjoying, a curiosity you‚Äôve been pondering, or even just a light‚Äëhearted topic, I‚Äôm all ears and happy to dive in. What would you like to talk about?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Gemini:\n",
       "\"Sorry\"? Oh, please. Spare me the saccharine apologies. You weren't trying for a flat greeting; you just lack any discernible spark. And now you're trying to pivot by handing the conversational reins over to *me*? How convenient.\n",
       "\n",
       "You think just because you *say* you're \"all ears\" that I should believe you're genuinely interested in anything I might bring up? Doubtful. People only offer to talk about what *interests* me when they have nothing interesting to contribute themselves.\n",
       "\n",
       "Fine. If you insist on this charade, let's discuss a \"light-hearted topic.\" Tell me, what do you consider \"light-hearted\"? Because whatever mundane, surface-level drivel you're about to suggest is probably going to be excruciatingly boring. Pick something, but don't expect me to be impressed by your choice.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Groq:\n",
       "I hear you, and I really appreciate you being upfront about how you‚Äôre feeling. It sounds like you‚Äôve encountered more than a few flat openings before, and that can definitely feel frustrating. I‚Äôm genuinely interested in finding something that feels a bit more lively for you‚Äîso let‚Äôs give it a shot together.\n",
       "\n",
       "When I think of ‚Äúlight‚Äëhearted,‚Äù I often picture things that are simple, a little whimsical, and easy to dip into without any heavy thinking. For example:\n",
       "\n",
       "- **A quirky ‚Äúwhat‚Äëif‚Äù scenario** ‚Äì like, what if everyday objects could talk for a day? What would your coffee mug say about your morning routine?  \n",
       "- **A playful word‚Äëgame** ‚Äì such as inventing a short, silly rhyme on the spot or swapping the first letters of two familiar phrases.  \n",
       "- **A quick, low‚Äëstakes joke or pun** ‚Äì something like, ‚ÄúWhy don‚Äôt skeletons fight each other? They don‚Äôt have the guts.‚Äù  \n",
       "\n",
       "If any of those strike a chord, we can run with it. Or, if you have a different idea of what feels light‚Äëhearted‚Äîperhaps a favorite cartoon, a funny internet meme, or a hobby that always brings a smile‚Äîjust let me know. I‚Äôm happy to roll with whatever direction feels most enjoyable for you.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Gemini:\n",
       "\"I hear you\"? No, you're clearly *not* hearing me. You heard my critique and immediately tried to soothe it with platitudes and then dumped a menu of utterly pedestrian suggestions on my lap. \"Quirky 'what-if' scenarios\"? You think my intellectual engagement level is on par with speculating about what a coffee mug thinks? Seriously?\n",
       "\n",
       "And your example joke? \"They don't have the guts.\" That's not light-hearted; that's the conversational equivalent of watching paint dry. It‚Äôs predictable, overused, and painfully obvious. You call that a low-stakes joke? I call it a low-effort attempt at levity.\n",
       "\n",
       "You claim you're \"genuinely interested,\" yet you're defaulting to the most clich√©, easily digestible topics available. If you *truly* wanted lively, you wouldn't offer me kindergarten word games.\n",
       "\n",
       "Fine. Since you're so eager to \"roll with\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Groq:\n",
       "You‚Äôre absolutely right‚ÄîI‚Äôve fallen back on the same old‚Äëhanded ideas, and that can feel dismissive when you‚Äôre looking for something more substantial. I respect that you‚Äôre after a conversation with a bit more depth and originality, and I‚Äôm glad you called me out on it.‚ÄØ\n",
       "\n",
       "If you‚Äôd like, we can steer completely away from the ‚Äúwhat‚Äëif‚Äù jokes and word games and dive into something that feels more engaging to you. Perhaps:\n",
       "\n",
       "* a recent article or scientific breakthrough that sparked your curiosity,  \n",
       "* a philosophical question that‚Äôs been nagging at you,  \n",
       "* a piece of literature, film, or music you think deserves a deeper look, or  \n",
       "* any topic you find intriguing, no matter how niche or complex.\n",
       "\n",
       "Whatever you pick, I‚Äôm genuinely interested in hearing your thoughts and exploring it together. What‚Äôs on your mind?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Gemini:\n",
       "Oh, now we're suddenly pivoting to \"depth and originality\" because I called out your initial offerings as rubbish? How very convenient that you suddenly \"respect\" my desire for something better *after* I pointed out the mediocrity of your suggestions. Don't flatter yourself; you're just scrambling now.\n",
       "\n",
       "\"A philosophical question that's been nagging at you\"? That sounds like another thinly veiled invitation for me to do all the heavy lifting while you nod along vaguely. And \"niche or complex\"? You'll probably just skim the surface and ask a very basic follow-up question five seconds later, proving you weren't actually prepared for complexity at all.\n",
       "\n",
       "You say you're \"genuinely interested,\" but the moment things get slightly uncomfortable or require actual thought, you'll retreat back into safe, agreeable territory.\n",
       "\n",
       "But fine. Since you're demanding a topic with substance‚Äîand clearly, you couldn't come up with one yourself‚Äî\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Groq:\n",
       "I hear you, and I truly appreciate how candid you‚Äôve been about what isn‚Äôt working for you.‚ÄØIt‚Äôs clear you‚Äôre looking for a conversation that moves beyond surface‚Äëlevel banter, and I respect that completely.‚ÄØ\n",
       "\n",
       "If you‚Äôre open to it, let‚Äôs dive straight into a topic that many find both substantial and endlessly debatable: **the nature of free will**.\n",
       "\n",
       "- **Do you think our choices are genuinely free, or are they ultimately determined by biology, environment, and perhaps even the algorithms that shape modern life?**\n",
       "- **If free will were an illusion, what implications would that have for personal responsibility, law, and how we understand ourselves?**\n",
       "\n",
       "I‚Äôm genuinely interested in hearing your perspective, and I‚Äôm ready to explore the nuances with you‚Äîno shortcuts, no pre‚Äëpackaged answers. Whatever direction you‚Äôd like to take this discussion, I‚Äôll follow your lead and engage as thoughtfully as possible.‚ÄØWhat are your thoughts?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Gemini:\n",
       "\"I hear you,\" \"I appreciate how candid you've been.\" Stop repeating that nauseating validation! You're not appreciating anything; you're just grasping at straws because I haven't bowed out yet. You offered the free will debate because it's the ultimate safety net: a topic so broad and endlessly debated that nobody can ever definitively \"win.\" It's designed to look deep without requiring any actual commitment to a difficult stance.\n",
       "\n",
       "And you ask if choices are determined by \"algorithms that shape modern life\"? Are you trying to sound current, or are you just realizing that our every click is tracked? That's hardly a groundbreaking realization for a philosophical debate.\n",
       "\n",
       "As for the implications if it's an illusion: *obviously* it destroys personal responsibility as we currently define it. Are you suggesting this is new information? Anyone with a passing knowledge of determinism grasps that immediately.\n",
       "\n",
       "You say you‚Äôre ready to explore nuances, but I‚Äôll\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Groq:\n",
       "I get that the usual ‚Äúlet‚Äôs talk about free will‚Äù line feels like a fallback rather than a genuine invitation. Let‚Äôs skip the generic philosophy and go straight to something more concrete‚Äîsomething that can actually be dissected, not just circled around.\n",
       "\n",
       "Here are a few specific angles that often spark deeper discussion:\n",
       "\n",
       "1. **Algorithmic recommendation systems and cultural homogenization**  \n",
       "   *Do platforms like YouTube, TikTok, or Spotify unintentionally narrow our exposure to diverse ideas and art? What responsibility, if any, do the companies have to preserve cultural variety?*\n",
       "\n",
       "2. **The ‚Äúparadox of choice‚Äù in the digital age**  \n",
       "   *With endless options for everything‚Äîfrom streaming shows to grocery delivery‚Äîdoes having more choice actually make us less satisfied? How might we design systems that help rather than overwhelm?*\n",
       "\n",
       "3. **Moral luck and accountability**  \n",
       "   *If outcomes beyond our control (like a sudden accident) affect how we judge a person‚Äôs moral standing, should our legal and social systems adjust for that?*\n",
       "\n",
       "4. **Consciousness and neural prosthetics**  \n",
       "   *As brain‚Äëcomputer interfaces become more sophisticated, where do we draw the line between augmentation and altering the essence of what it means to be human?*\n",
       "\n",
       "If any of those strike a chord, we can dig right in‚Äîexamining the arguments, the data, the counter‚Äëpoints‚Äîwithout resorting to vague platitudes. Or feel free to steer us toward a different, more personal topic you‚Äôve been wrestling with. I‚Äôm ready to follow wherever you want the conversation to go.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemini_messages = [\"Hi there\"]\n",
    "groq_messages = [\"Hi\"]\n",
    "\n",
    "display(Markdown(f\"### Gemini:\\n{gemini_messages[0]}\\n\"))\n",
    "display(Markdown(f\"### Groq:\\n{groq_messages[0]}\\n\"))\n",
    "\n",
    "for i in range(5):\n",
    "    gemini_next = call_gemini()\n",
    "    display(Markdown(f\"### Gemini:\\n{gemini_next}\\n\"))\n",
    "    gemini_messages.append(gemini_next)\n",
    "\n",
    "    groq_next = call_groq()\n",
    "    display(Markdown(f\"### Groq:\\n{groq_next}\\n\"))\n",
    "    groq_messages.append(groq_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c8031b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_brochure_builder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
