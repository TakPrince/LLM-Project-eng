{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec504701",
   "metadata": {},
   "source": [
    "# Let's go PRO!\n",
    "\n",
    "Advanced RAG Techniques!\n",
    "\n",
    "Let's start by digging into ingest:\n",
    "\n",
    "1. No LangChain! Just native for maximum flexibility\n",
    "2. Let's use an LLM to divide up chunks in a sensible way\n",
    "3. Let's use the best chunk size and encoder from yesterday\n",
    "4. Let's also have the LLM rewrite chunks in a way that's most useful (\"document pre-processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "262d0b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from groq import Groq          # üîÅ changed\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from chromadb import PersistentClient\n",
    "from tqdm import tqdm\n",
    "from litellm import completion\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "MODEL = \"groq/openai/gpt-oss-120b\"      # üîÅ Groq model\n",
    "\n",
    "DB_NAME = \"preprocessed_db\"\n",
    "collection_name = \"docs\"\n",
    "embedding_model = \"text-embedding-3-large\"\n",
    "KNOWLEDGE_BASE_PATH = Path(\"knowledge-base\")\n",
    "AVERAGE_CHUNK_SIZE = 500\n",
    "\n",
    "groq = Groq()                 # üîÅ Groq client\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3a0b06af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by LangChain's Document - let's have something similar\n",
    "\n",
    "class Result(BaseModel):\n",
    "    page_content: str\n",
    "    metadata: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b382ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to perfectly represent a chunk\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    headline: str = Field(description=\"A brief heading for this chunk, typically a few words, that is most likely to be surfaced in a query\")\n",
    "    summary: str = Field(description=\"A few sentences summarizing the content of this chunk to answer common questions\")\n",
    "    original_text: str = Field(description=\"The original text of this chunk from the provided document, exactly as is, not changed in any way\")\n",
    "\n",
    "    def as_result(self, document):\n",
    "        metadata = {\"source\": document[\"source\"], \"type\": document[\"type\"]}\n",
    "        return Result(page_content=self.headline + \"\\n\\n\" + self.summary + \"\\n\\n\" + self.original_text,metadata=metadata)\n",
    "\n",
    "\n",
    "class Chunks(BaseModel):\n",
    "    chunks: list[Chunk]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afae8e31",
   "metadata": {},
   "source": [
    "## Three steps:\n",
    "\n",
    "1. Fetch documents from the knowledge base, like LangChain did\n",
    "2. Call an LLM to turn documents into Chunks\n",
    "3. Store the Chunks in Chroma\n",
    "\n",
    "### Let's start with Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e346885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_documents():\n",
    "    \"\"\"A homemade version of the LangChain DirectoryLoader\"\"\"\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for folder in KNOWLEDGE_BASE_PATH.iterdir():\n",
    "        doc_type = folder.name\n",
    "        for file in folder.rglob(\"*.md\"):\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                documents.append({\"type\": doc_type, \"source\": file.as_posix(), \"text\": f.read()})\n",
    "\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e145ae8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 76 documents\n"
     ]
    }
   ],
   "source": [
    "documents = fetch_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9846837",
   "metadata": {},
   "source": [
    "### On to Step 2 - make the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3e07c3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_prompt(document):\n",
    "#     how_many = (len(document[\"text\"]) // AVERAGE_CHUNK_SIZE) + 1\n",
    "#     return f\"\"\"\n",
    "# You take a document and you split the document into overlapping chunks for a KnowledgeBase.\n",
    "\n",
    "# The document is from the shared drive of a company called Insurellm.\n",
    "# The document is of type: {document[\"type\"]}\n",
    "# The document has been retrieved from: {document[\"source\"]}\n",
    "\n",
    "# A chatbot will use these chunks to answer questions about the company.\n",
    "# You should divide up the document as you see fit, being sure that the entire document is returned in the chunks - don't leave anything out.\n",
    "# This document should probably be split into {how_many} chunks, but you can have more or less as appropriate.\n",
    "# There should be overlap between the chunks as appropriate; typically about 25% overlap or about 50 words, so you have the same text in multiple chunks for best retrieval results.\n",
    "\n",
    "# For each chunk, you should provide a headline, a summary, and the original text of the chunk.\n",
    "# Together your chunks should represent the entire document with overlap.\n",
    "\n",
    "# Here is the document:\n",
    "\n",
    "# {document[\"text\"]}\n",
    "\n",
    "# Respond with the chunks.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "def make_prompt(document):\n",
    "    how_many = (len(document[\"text\"]) // AVERAGE_CHUNK_SIZE) + 1\n",
    "    return f\"\"\"\n",
    "You take a document and you split the document into overlapping chunks for a KnowledgeBase.\n",
    "\n",
    "The document is from the shared drive of a company called Insurellm.\n",
    "The document is of type: {document[\"type\"]}\n",
    "The document has been retrieved from: {document[\"source\"]}\n",
    "\n",
    "A chatbot will use these chunks to answer questions about the company.\n",
    "You should divide up the document as you see fit, being sure that the entire document is returned in the chunks.\n",
    "This document should probably be split into {how_many} chunks.\n",
    "\n",
    "Return a JSON object with a single key \"chunks\" containing the list of chunk objects.\n",
    "Example: {{\"chunks\": [{{...}}, {{...}}]}}\n",
    "\n",
    "Here is the document:\n",
    "\n",
    "{document[\"text\"]}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "085546fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You take a document and you split the document into overlapping chunks for a KnowledgeBase.\n",
      "\n",
      "The document is from the shared drive of a company called Insurellm.\n",
      "The document is of type: contracts\n",
      "The document has been retrieved from: knowledge-base/contracts/Contract with Belvedere Insurance for Markellm.md\n",
      "\n",
      "A chatbot will use these chunks to answer questions about the company.\n",
      "You should divide up the document as you see fit, being sure that the entire document is returned in the chunks.\n",
      "This document should probably be split into 7 chunks.\n",
      "\n",
      "Return a JSON object with a single key \"chunks\" containing the list of chunk objects.\n",
      "Example: {\"chunks\": [{...}, {...}]}\n",
      "\n",
      "Here is the document:\n",
      "\n",
      "# Contract with Belvedere Insurance for Markellm\n",
      "\n",
      "## Terms\n",
      "This Contract (\"Agreement\") is made and entered into as of [Date] by and between Insurellm, Inc., a corporation registered in the United States, (\"Provider\") and Belvedere Insurance, (\"Client\"). \n",
      "\n",
      "1. **Service Commencement**: The services described herein will commence on [Start Date].\n",
      "2. **Contract Duration**: This Agreement shall remain in effect for a period of 1 year from the Commencement Date, unless terminated earlier in accordance with the termination clause of this Agreement.\n",
      "3. **Fees**: Client agrees to pay a Basic Listing Fee of $199/month for accessing the Markellm platform along with a performance-based pricing of $25 per lead generated.\n",
      "4. **Payment Terms**: Payments shall be made monthly, in advance, with invoices issued on the 1st of each month, payable within 15 days of receipt.\n",
      "\n",
      "## Renewal\n",
      "1. **Renewal Terms**: This Agreement may be renewed for additional one-year terms upon mutual written consent of both parties no later than 30 days before the end of the current term.\n",
      "2. **Fee Adjustments**: Any changes to the fees or terms will be communicated in writing at least 60 days prior to the renewal date.\n",
      "\n",
      "## Features\n",
      "1. **AI-Powered Matching**: Belvedere Insurance will benefit from Markellm's AI-powered matching, ensuring the best-fit customers are identified and connected.\n",
      "2. **Real-Time Quotes**: Access to real-time quotes will enhance the customer acquisition process, facilitating timely and informed decision-making.\n",
      "3. **Data Insights**: Client shall have access to Markellm's analytics dashboard, allowing insights into consumer behavior and market trends.\n",
      "4. **Customization Options**: Belvedere Insurance can leverage optional premium features and analytics upon payment of an additional $9.99/month.\n",
      "5. **Customer Support**: Insurellm will provide dedicated support to Belvedere Insurance, ensuring any issues or queries are promptly addressed.\n",
      "\n",
      "## Support\n",
      "1. **Technical Support**: Technical support will be available from 9 AM to 7 PM EST, Monday through Friday via email and phone.\n",
      "2. **Response Times**: Insurellm agrees to respond to all support queries within 24 business hours. Emergency support will be prioritized throughout the contract period.\n",
      "3. **Training**: Insurellm will offer a comprehensive training session for the Client‚Äôs staff upon beginning the service to ensure effective utilization of the features.\n",
      "\n",
      "## Acceptance\n",
      "By signing below, the parties agree to the terms of this Agreement.\n",
      "\n",
      "**Insurellm, Inc.**  \n",
      "Signature: ______________________  \n",
      "Name: [Authorized Signatory]  \n",
      "Title: [Title]  \n",
      "Date: ______________________  \n",
      "\n",
      "**Belvedere Insurance**  \n",
      "Signature: ______________________  \n",
      "Name: [Authorized Signatory]  \n",
      "Title: [Title]  \n",
      "Date: ______________________  \n",
      "\n",
      "--- \n",
      "This synthetic contract document outlines a fictional agreement between Insurellm and a fictional insurance client, Belvedere Insurance, which engages with the Markellm platform. The contract contains creative yet realistic terms for potential use in training and development in insurance technology scenarios.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(make_prompt(documents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d8efd3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_messages(document):\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": make_prompt(document)},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5cec877e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': '\\nYou take a document and you split the document into overlapping chunks for a KnowledgeBase.\\n\\nThe document is from the shared drive of a company called Insurellm.\\nThe document is of type: contracts\\nThe document has been retrieved from: knowledge-base/contracts/Contract with Belvedere Insurance for Markellm.md\\n\\nA chatbot will use these chunks to answer questions about the company.\\nYou should divide up the document as you see fit, being sure that the entire document is returned in the chunks.\\nThis document should probably be split into 7 chunks.\\n\\nReturn a JSON object with a single key \"chunks\" containing the list of chunk objects.\\nExample: {\"chunks\": [{...}, {...}]}\\n\\nHere is the document:\\n\\n# Contract with Belvedere Insurance for Markellm\\n\\n## Terms\\nThis Contract (\"Agreement\") is made and entered into as of [Date] by and between Insurellm, Inc., a corporation registered in the United States, (\"Provider\") and Belvedere Insurance, (\"Client\"). \\n\\n1. **Service Commencement**: The services described herein will commence on [Start Date].\\n2. **Contract Duration**: This Agreement shall remain in effect for a period of 1 year from the Commencement Date, unless terminated earlier in accordance with the termination clause of this Agreement.\\n3. **Fees**: Client agrees to pay a Basic Listing Fee of $199/month for accessing the Markellm platform along with a performance-based pricing of $25 per lead generated.\\n4. **Payment Terms**: Payments shall be made monthly, in advance, with invoices issued on the 1st of each month, payable within 15 days of receipt.\\n\\n## Renewal\\n1. **Renewal Terms**: This Agreement may be renewed for additional one-year terms upon mutual written consent of both parties no later than 30 days before the end of the current term.\\n2. **Fee Adjustments**: Any changes to the fees or terms will be communicated in writing at least 60 days prior to the renewal date.\\n\\n## Features\\n1. **AI-Powered Matching**: Belvedere Insurance will benefit from Markellm\\'s AI-powered matching, ensuring the best-fit customers are identified and connected.\\n2. **Real-Time Quotes**: Access to real-time quotes will enhance the customer acquisition process, facilitating timely and informed decision-making.\\n3. **Data Insights**: Client shall have access to Markellm\\'s analytics dashboard, allowing insights into consumer behavior and market trends.\\n4. **Customization Options**: Belvedere Insurance can leverage optional premium features and analytics upon payment of an additional $9.99/month.\\n5. **Customer Support**: Insurellm will provide dedicated support to Belvedere Insurance, ensuring any issues or queries are promptly addressed.\\n\\n## Support\\n1. **Technical Support**: Technical support will be available from 9 AM to 7 PM EST, Monday through Friday via email and phone.\\n2. **Response Times**: Insurellm agrees to respond to all support queries within 24 business hours. Emergency support will be prioritized throughout the contract period.\\n3. **Training**: Insurellm will offer a comprehensive training session for the Client‚Äôs staff upon beginning the service to ensure effective utilization of the features.\\n\\n## Acceptance\\nBy signing below, the parties agree to the terms of this Agreement.\\n\\n**Insurellm, Inc.**  \\nSignature: ______________________  \\nName: [Authorized Signatory]  \\nTitle: [Title]  \\nDate: ______________________  \\n\\n**Belvedere Insurance**  \\nSignature: ______________________  \\nName: [Authorized Signatory]  \\nTitle: [Title]  \\nDate: ______________________  \\n\\n--- \\nThis synthetic contract document outlines a fictional agreement between Insurellm and a fictional insurance client, Belvedere Insurance, which engages with the Markellm platform. The contract contains creative yet realistic terms for potential use in training and development in insurance technology scenarios.\\n'}]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_messages(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c4f7cfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "GROQ_KEYS = os.getenv('GROQ_API_KEY2') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e237179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_document(document):\n",
    "#     messages = make_messages(document)\n",
    "#     response = completion(model=MODEL, messages=messages, response_format=Chunks)\n",
    "#     reply = response.choices[0].message.content\n",
    "#     doc_as_chunks = Chunks.model_validate_json(reply).chunks\n",
    "#     return [chunk.as_result(document) for chunk in doc_as_chunks]\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "def process_document(document):\n",
    "    messages = make_messages(document)\n",
    "    \n",
    "    # NEW: Added num_retries to handle transient rate limits automatically\n",
    "    response = completion(\n",
    "        model=MODEL, \n",
    "        messages=messages, \n",
    "        response_format=Chunks,\n",
    "        api_key=GROQ_KEYS,\n",
    "        num_retries=3  # litellm will retry 3 times with backoff\n",
    "    )\n",
    "    \n",
    "    reply = response.choices[0].message.content\n",
    "    doc_as_chunks = Chunks.model_validate_json(reply).chunks\n",
    "    return [chunk.as_result(document) for chunk in doc_as_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b42f3b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Result(page_content='Contract Title and Parties\\n\\nIntroduces the contract between Insurellm and Belvedere Insurance, stating the parties involved and the effective date placeholder.\\n\\n# Contract with Belvedere Insurance for Markellm\\n\\n## Terms\\nThis Contract (\"Agreement\") is made and entered into as of [Date] by and between Insurellm, Inc., a corporation registered in the United States, (\"Provider\") and Belvedere Insurance, (\"Client\"). ', metadata={'source': 'knowledge-base/contracts/Contract with Belvedere Insurance for Markellm.md', 'type': 'contracts'}),\n",
       " Result(page_content='Key Terms: Service, Duration, Fees, Payment\\n\\nDetails service start date, one‚Äëyear duration, fee structure ($199/month plus $25 per lead), and monthly payment schedule.\\n\\n1. **Service Commencement**: The services described herein will commence on [Start Date].\\n2. **Contract Duration**: This Agreement shall remain in effect for a period of 1 year from the Commencement Date, unless terminated earlier in accordance with the termination clause of this Agreement.\\n3. **Fees**: Client agrees to pay a Basic Listing Fee of $199/month for accessing the Markellm platform along with a performance-based pricing of $25 per lead generated.\\n4. **Payment Terms**: Payments shall be made monthly, in advance, with invoices issued on the 1st of each month, payable within 15 days of receipt.', metadata={'source': 'knowledge-base/contracts/Contract with Belvedere Insurance for Markellm.md', 'type': 'contracts'}),\n",
       " Result(page_content='Renewal Terms\\n\\nExplains renewal option for additional one‚Äëyear terms with 30‚Äëday notice and fee adjustment notice period of 60 days.\\n\\n## Renewal\\n1. **Renewal Terms**: This Agreement may be renewed for additional one-year terms upon mutual written consent of both parties no later than 30 days before the end of the current term.\\n2. **Fee Adjustments**: Any changes to the fees or terms will be communicated in writing at least 60 days prior to the renewal date.', metadata={'source': 'knowledge-base/contracts/Contract with Belvedere Insurance for Markellm.md', 'type': 'contracts'}),\n",
       " Result(page_content=\"Core Features ‚Äì AI Matching, Real‚ÄëTime Quotes, Data Insights\\n\\nDescribes AI‚Äëpowered matching, real‚Äëtime quoting, and analytics dashboard providing consumer behavior insights.\\n\\n## Features\\n1. **AI-Powered Matching**: Belvedere Insurance will benefit from Markellm's AI-powered matching, ensuring the best-fit customers are identified and connected.\\n2. **Real-Time Quotes**: Access to real-time quotes will enhance the customer acquisition process, facilitating timely and informed decision-making.\\n3. **Data Insights**: Client shall have access to Markellm's analytics dashboard, allowing insights into consumer behavior and market trends.\", metadata={'source': 'knowledge-base/contracts/Contract with Belvedere Insurance for Markellm.md', 'type': 'contracts'}),\n",
       " Result(page_content='Additional Features and Support Overview\\n\\nCovers customization option ($9.99/month) and dedicated customer support, introducing the support section.\\n\\n4. **Customization Options**: Belvedere Insurance can leverage optional premium features and analytics upon payment of an additional $9.99/month.\\n5. **Customer Support**: Insurellm will provide dedicated support to Belvedere Insurance, ensuring any issues or queries are promptly addressed.\\n\\n## Support', metadata={'source': 'knowledge-base/contracts/Contract with Belvedere Insurance for Markellm.md', 'type': 'contracts'}),\n",
       " Result(page_content='Support Services Details\\n\\nSpecifies technical support hours, 24‚Äëhour response time policy, and training session for client staff.\\n\\n1. **Technical Support**: Technical support will be available from 9 AM to 7 PM EST, Monday through Friday via email and phone.\\n2. **Response Times**: Insurellm agrees to respond to all support queries within 24 business hours. Emergency support will be prioritized throughout the contract period.\\n3. **Training**: Insurellm will offer a comprehensive training session for the Client‚Äôs staff upon beginning the service to ensure effective utilization of the features.', metadata={'source': 'knowledge-base/contracts/Contract with Belvedere Insurance for Markellm.md', 'type': 'contracts'}),\n",
       " Result(page_content='Acceptance and Disclaimer\\n\\nProvides signature placeholders for both parties and a disclaimer noting the synthetic nature of the contract.\\n\\n## Acceptance\\nBy signing below, the parties agree to the terms of this Agreement.\\n\\n**Insurellm, Inc.**  \\nSignature: ______________________  \\nName: [Authorized Signatory]  \\nTitle: [Title]  \\nDate: ______________________  \\n\\n**Belvedere Insurance**  \\nSignature: ______________________  \\nName: [Authorized Signatory]  \\nTitle: [Title]  \\nDate: ______________________  \\n\\n--- \\nThis synthetic contract document outlines a fictional agreement between Insurellm and a fictional insurance client, Belvedere Insurance, which engages with the Markellm platform. The contract contains creative yet realistic terms for potential use in training and development in insurance technology scenarios.', metadata={'source': 'knowledge-base/contracts/Contract with Belvedere Insurance for Markellm.md', 'type': 'contracts'})]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_document(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "8ace6ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_chunks(documents):\n",
    "#     chunks = []\n",
    "#     for doc in tqdm(documents):\n",
    "#         chunks.extend(process_document(doc))\n",
    "#         # time.sleep(2)   # prevent token burst\n",
    "#     return chunks\n",
    "\n",
    "def create_chunks(documents):\n",
    "    chunks = []\n",
    "    for doc in tqdm(documents):\n",
    "        chunks.extend(process_document(doc))\n",
    "        \n",
    "        # NEW: Mandatory sleep to avoid hitting the 8000 TPM limit\n",
    "        # 15-20 seconds is safer for the Groq Free/On-demand tier\n",
    "        time.sleep(20) \n",
    "        \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8b573e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|‚ñà‚ñà‚ñà‚ñå      | 27/76 [11:58<23:50, 29.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 29/76 [13:03<23:33, 30.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 42/76 [18:17<13:31, 23.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 43/76 [20:07<27:25, 49.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 45/76 [24:39<43:15, 83.72s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 46/76 [26:29<45:52, 91.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 47/76 [28:20<47:04, 97.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 49/76 [30:34<35:07, 78.07s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 50/76 [32:24<38:02, 87.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "Attempt 3 failed: litellm.InternalServerError: InternalServerError: GroqException - [Errno -3] Temporary failure in name resolution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 51/76 [33:45<35:39, 85.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 52/76 [35:42<37:57, 94.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 53/76 [36:44<32:37, 85.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 54/76 [38:34<33:59, 92.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 55/76 [42:14<45:44, 130.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 56/76 [44:04<41:32, 124.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 57/76 [45:55<38:07, 120.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 58/76 [47:45<35:14, 117.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 59/76 [49:41<33:06, 116.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 60/76 [51:31<30:39, 114.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 61/76 [53:22<28:24, 113.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 63/76 [55:36<18:39, 86.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 64/76 [57:27<18:41, 93.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 65/76 [59:17<18:04, 98.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 66/76 [1:01:08<17:01, 102.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 67/76 [1:03:02<15:51, 105.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 68/76 [1:04:52<14:17, 107.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 69/76 [1:06:18<11:44, 100.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 70/76 [1:08:08<10:21, 103.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 71/76 [1:09:59<08:48, 105.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 73/76 [1:12:14<04:07, 82.48s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 74/76 [1:14:05<03:01, 90.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 75/76 [1:15:55<01:36, 96.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 76/76 [1:17:46<00:00, 61.40s/it] \n"
     ]
    }
   ],
   "source": [
    "chunks = create_chunks(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3056cc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (5.2.0)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: tqdm in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (3.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2026.1.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/asus/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a9e2e3",
   "metadata": {},
   "source": [
    "### Well that was easy! If a bit slow.\n",
    "\n",
    "In the python module version, I sneakily use the multi-processing Pool to run this in parallel,\n",
    "but if you get a Rate Limit Error you can turn this off in the code.\n",
    "\n",
    "### Finally, Step 3 - save the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2633a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_embeddings(chunks):\n",
    "#     chroma = PersistentClient(path=DB_NAME)\n",
    "#     if collection_name in [c.name for c in chroma.list_collections()]:\n",
    "#         chroma.delete_collection(collection_name)\n",
    "\n",
    "#     texts = [chunk.page_content for chunk in chunks]\n",
    "#     emb = openai.embeddings.create(model=embedding_model, input=texts).data\n",
    "#     vectors = [e.embedding for e in emb]\n",
    "\n",
    "#     collection = chroma.get_or_create_collection(collection_name)\n",
    "\n",
    "#     ids = [str(i) for i in range(len(chunks))]\n",
    "#     metas = [chunk.metadata for chunk in chunks]\n",
    "\n",
    "#     collection.add(ids=ids, embeddings=vectors, documents=texts, metadatas=metas)\n",
    "#     print(f\"Vectorstore created with {collection.count()} documents\")\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# load model once (global)\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def create_embeddings(chunks):\n",
    "    chroma = PersistentClient(path=DB_NAME)\n",
    "    if collection_name in [c.name for c in chroma.list_collections()]:\n",
    "        chroma.delete_collection(collection_name)\n",
    "\n",
    "    texts = [chunk.page_content for chunk in chunks]\n",
    "\n",
    "    # create embeddings locally (no API)\n",
    "    vectors = embedding_model.encode(texts, show_progress_bar=True).tolist()\n",
    "\n",
    "    collection = chroma.get_or_create_collection(collection_name)\n",
    "\n",
    "    ids = [str(i) for i in range(len(chunks))]\n",
    "    metas = [chunk.metadata for chunk in chunks]\n",
    "\n",
    "    collection.add(ids=ids, embeddings=vectors, documents=texts, metadatas=metas)\n",
    "    print(f\"Vectorstore created with {collection.count()} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "5b6ab7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd902c2003bd4d6c88ad6b3244597c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 272 documents\n"
     ]
    }
   ],
   "source": [
    "create_embeddings(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0ab732",
   "metadata": {},
   "source": [
    "# Visual represnetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f216decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma = PersistentClient(path=DB_NAME)\n",
    "collection = chroma.get_or_create_collection(collection_name)\n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "metadatas = result['metadatas']\n",
    "doc_types = [metadata['type'] for metadata in metadatas]\n",
    "colors = [['blue', 'green', 'red', 'orange'][['products', 'employees', 'contracts', 'company'].index(t)] for t in doc_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e71f3546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "text",
         "marker": {
          "color": [
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "orange",
           "orange",
           "orange"
          ],
          "opacity": 0.8,
          "size": 5
         },
         "mode": "markers",
         "text": [
          "Type: contracts<br>Text: Contract Terms\n\nThis section establishes the parties and outlines the core service provisions, durat...",
          "Type: contracts<br>Text: Renewal Terms\n\nThe renewal clause details how the agreement may be extended for additional one‚Äëyear ...",
          "Type: contracts<br>Text: Platform Features\n\nThis section enumerates the AI‚Äëdriven matching, real‚Äëtime quoting, analytics, opt...",
          "Type: contracts<br>Text: Support Services\n\nThe support provisions define technical support hours, response time commitments, ...",
          "Type: contracts<br>Text: Agreement Acceptance\n\nThe acceptance section provides signature lines for both parties, confirming t...",
          "Type: contracts<br>Text: Terms and Parties\n\nThe agreement defines the parties, product description, payment terms, and usage ...",
          "Type: contracts<br>Text: Renewal Terms\n\nThe contract automatically renews yearly unless a 60‚Äëday termination notice is given....",
          "Type: contracts<br>Text: Key Features\n\nRellm offers AI‚Äëdriven analytics, a customizable dashboard, compliance tools, and clie...",
          "Type: contracts<br>Text: Support and Services\n\nInsurellm provides 24/7 customer support, initial training, free updates, and ...",
          "Type: contracts<br>Text: Terms\n\nThe contract becomes effective on January 1, 2025 and runs for a 12‚Äëmonth term ending Decembe...",
          "Type: contracts<br>Text: Renewal\n\nThe agreement will auto‚Äërenew for another 12‚Äëmonth period unless either party gives a 30‚Äëda...",
          "Type: contracts<br>Text: Features\n\nRoadway Insurance Inc. gains access to all Professional Tier capabilities, including AI‚Äëpo...",
          "Type: contracts<br>Text: Support\n\nThe contract provides priority technical support, up to five training sessions, and quarter...",
          "Type: contracts<br>Text: Terms of Agreement\n\nThe contract outlines coverage, duration, payment, and overage fees for the Clai...",
          "Type: contracts<br>Text: Renewal Provisions\n\nThe agreement automatically renews for successive 12‚Äëmonth terms unless a 30‚Äëday...",
          "Type: contracts<br>Text: Core Tier Features\n\nRapid Claims Associates receives a suite of AI‚Äëdriven claim processing tools, in...",
          "Type: contracts<br>Text: Support Services\n\nInsurellm provides onboarding, technical support, regular platform updates, and de...",
          "Type: contracts<br>Text: Signatures & Final Clause\n\nThe contract is signed by senior representatives of Insurellm and Rapid C...",
          "Type: contracts<br>Text: Contract Overview\n\nThis contract, dated February 12, 2025, establishes an agreement between Insurell...",
          "Type: contracts<br>Text: Terms\n\nThe Terms section defines the parties, scope of services, payment schedule, policy volume, an...",
          "Type: contracts<br>Text: Renewal\n\nThe Renewal clause specifies automatic 12‚Äëmonth renewals unless a 45‚Äëday notice is given, a...",
          "Type: contracts<br>Text: Features\n\nThe Features section details AI‚Äëpowered underwriting, predictive risk modeling, digital he...",
          "Type: contracts<br>Text: Support\n\nSupport provisions include priority technical assistance, a comprehensive training program,...",
          "Type: contracts<br>Text: Contract Overview\n\nThe contract between Insurellm and GreenField Holdings becomes effective on Novem...",
          "Type: contracts<br>Text: Terms of Agreement\n\nThe agreement defines Insurellm as the Provider and GreenField Holdings as the C...",
          "Type: contracts<br>Text: Renewal Provisions\n\nThe contract will automatically renew for one‚Äëyear terms unless a written non‚Äëre...",
          "Type: contracts<br>Text: Platform Features\n\nThe contract lists four core features of the Markellm platform, from AI‚Äëpowered m...",
          "Type: contracts<br>Text: Support and Training\n\nInsurellm will provide the client with phone and email support, onboarding tra...",
          "Type: contracts<br>Text: Pricing and Signatures\n\nPricing consists of a fixed monthly listing fee and a performance‚Äëbased char...",
          "Type: contracts<br>Text: Contract Overview and Core Terms\n\nThis section outlines the contract date, number, parties, and the ...",
          "Type: contracts<br>Text: Renewal Terms and Pricing Adjustments\n\nThe renewal clause defines a 120‚Äëday notice period, most‚Äëfavo...",
          "Type: contracts<br>Text: Key Platform Features ‚Äì Member, Multi‚ÄëState, Branding, AI Claims\n\nThis chunk details the primary cap...",
          "Type: contracts<br>Text: Extended Feature Set ‚Äì Health Management, APIs, Network, Medication, Engagement, Quality, Analytics,...",
          "Type: contracts<br>Text: Comprehensive Support Services\n\nThe support chapter defines a dedicated success team, 24/7 premium s...",
          "Type: contracts<br>Text: Signatures and Final Provisions\n\nThe concluding section records the authorized signatures of both pa...",
          "Type: contracts<br>Text: Terms\n\nThe contract becomes effective on March‚ÄØ1‚ÄØ2025 and runs for 24 months until February‚ÄØ28‚ÄØ2027....",
          "Type: contracts<br>Text: Renewal\n\nThe agreement auto‚Äërenews for another 24‚Äëmonth term unless a 60‚Äëday notice is given, with p...",
          "Type: contracts<br>Text: Features\n\nGuardian gains full access to Growth Tier capabilities, including AI‚Äëdriven underwriting, ...",
          "Type: contracts<br>Text: Support\n\nComprehensive support includes priority technical assistance, extensive training programs, ...",
          "Type: contracts<br>Text: Signatures\n\nThe contract is signed by Michael Torres, CRO of Insurellm, Inc., and Jonathan Park, Pre...",
          "Type: contracts<br>Text: Terms\n\nThis section defines the parties, license grant, payment obligations, and the initial two‚Äëyea...",
          "Type: contracts<br>Text: Renewal\n\nThe contract automatically renews for one‚Äëyear periods unless notice is given, and subscrip...",
          "Type: contracts<br>Text: Features\n\nThe product provides AI‚Äëdriven risk assessment, dynamic pricing, rapid claim processing, p...",
          "Type: contracts<br>Text: Support\n\nInsurellm commits to 24/7 technical support, onsite and webinar training, and regular syste...",
          "Type: contracts<br>Text: Contract Overview and Core Terms\n\nThis chunk outlines the contract date, parties, and key terms such...",
          "Type: contracts<br>Text: Liability, SLA, Exclusivity and Renewal Provisions\n\nThis chunk details liability limits, SLA uptime ...",
          "Type: contracts<br>Text: Feature Set Delivered to Continental Commercial Group\n\nThis chunk enumerates the comprehensive Bizll...",
          "Type: contracts<br>Text: Support Services, Signatures and Final Agreement\n\nThis chunk outlines the extensive support commitme...",
          "Type: contracts<br>Text: Contract Overview\n\nThe contract, dated April 20, 2025, establishes the agreement between Insurellm, ...",
          "Type: contracts<br>Text: Renewal Terms\n\nThe agreement includes a 120‚Äëday mutual renewal notice and guarantees that National C...",
          "Type: contracts<br>Text: Features Overview\n\nNational Claims Network receives the full Claimllm Enterprise suite, including un...",
          "Type: contracts<br>Text: Support Services\n\nInsurellm provides extensive enterprise‚Äëlevel support covering dedicated success t...",
          "Type: contracts<br>Text: Signatures and Closing\n\nThe contract is signed by Jennifer Rodriguez of Insurellm and Amanda Richard...",
          "Type: contracts<br>Text: Terms Overview\n\nThe contract becomes effective on February‚ÄØ1‚ÄØ2025 and runs for a 24‚Äëmonth term endin...",
          "Type: contracts<br>Text: Renewal Terms\n\nThe agreement automatically renews for another 24‚Äëmonth period unless a 60‚Äëday writte...",
          "Type: contracts<br>Text: Feature Set\n\nThe Professional Tier grants Fortress Business Underwriters access to a comprehensive s...",
          "Type: contracts<br>Text: Support and Maintenance\n\nInsurellm provides priority technical support, dedicated account management...",
          "Type: contracts<br>Text: Contract Overview and Parties\n\nThe agreement establishes Insurellm, Inc. as the Provider and Apex Re...",
          "Type: contracts<br>Text: Payment Terms and Contract Duration\n\nThe Client agrees to pay $10,000 monthly, with payments due on ...",
          "Type: contracts<br>Text: Renewal Provisions\n\nThe agreement automatically renews for one-year terms unless a thirty‚Äëday termin...",
          "Type: contracts<br>Text: Key Solution Features\n\nThe Rellm platform offers AI‚Äëdriven analytics, seamless system integrations, ...",
          "Type: contracts<br>Text: Support Services and Acceptance\n\nProvider delivers technical support, training for up to ten staff m...",
          "Type: contracts<br>Text: Terms Overview\n\nThe contract defines the parties, a non-exclusive license, payment schedule, claims ...",
          "Type: contracts<br>Text: Renewal Terms\n\nThe agreement automatically renews for successive 12‚Äëmonth periods unless a 45‚Äëday no...",
          "Type: contracts<br>Text: Features Summary\n\nFastTrack receives access to a comprehensive suite of AI‚Äëdriven claim processing f...",
          "Type: contracts<br>Text: Support Services\n\nInsurellm will provide priority customer support with defined response times and 2...",
          "Type: contracts<br>Text: Signatures\n\nThe contract is signed by Sarah Chen, VP of Sales for Insurellm, and Rebecca Martinez, C...",
          "Type: contracts<br>Text: Contract Overview\n\nThe contract establishes the agreement between Insurellm, Inc. and WellCare Insur...",
          "Type: contracts<br>Text: Terms Summary\n\nThe Terms section defines coverage, duration, payment, member limits, confidentiality...",
          "Type: contracts<br>Text: Renewal and Pricing\n\nThe Renewal clause states the agreement auto‚Äërenews for successive 12‚Äëmonth per...",
          "Type: contracts<br>Text: Features Overview\n\nThe Features section lists the Essential Tier capabilities, including intelligent...",
          "Type: contracts<br>Text: Support Services\n\nThe Support section describes onboarding, technical assistance, and service hours,...",
          "Type: contracts<br>Text: Signatures and Conclusion\n\nThe signatures confirm the agreement by senior executives of both compani...",
          "Type: contracts<br>Text: Terms\n\nThe contract becomes effective on Jan 25, 2025 and runs for 24 months ending Jan 24, 2027. It...",
          "Type: contracts<br>Text: Renewal\n\nThe agreement auto‚Äërenews for another 24‚Äëmonth term unless a 90‚Äëday notice is given. Price ...",
          "Type: contracts<br>Text: Features\n\nHarmony gains access to the Professional Tier‚Äôs AI‚Äëdriven tools, including plan design, el...",
          "Type: contracts<br>Text: Support Services\n\nHealthllm provides priority technical support, extensive training, regular updates...",
          "Type: contracts<br>Text: Signatures\n\nThe contract is signed by Sarah Chen of Insurellm, Inc. and Dr. Karen Phillips of Harmon...",
          "Type: contracts<br>Text: Contract Overview\n\nThe contract is established between Insurellm and Stellar Insurance Co., specifyi...",
          "Type: contracts<br>Text: Duration and Renewal\n\nThe agreement starts with a 12‚Äëmonth term and automatically renews for success...",
          "Type: contracts<br>Text: Payment Terms\n\nStellar Insurance Co. will pay a monthly subscription of $10,000 for the Professional...",
          "Type: contracts<br>Text: Termination\n\nEither party can end the contract with a 30‚Äëday written notice. A material breach allow...",
          "Type: contracts<br>Text: Features\n\nStellar Insurance Co. receives multiple Rellm features, including AI‚Äëdriven analytics, sea...",
          "Type: contracts<br>Text: Support Services\n\nInsurellm provides 24/7 technical support, quarterly account reviews, training ses...",
          "Type: contracts<br>Text: Signatures\n\nBoth parties acknowledge receipt of the product summary and agree to the contract terms....",
          "Type: contracts<br>Text: Terms\n\nThis section defines the parties, the services provided, contract duration, payment terms, an...",
          "Type: contracts<br>Text: Renewal\n\nThe renewal clause specifies automatic one-year extensions unless a 60‚Äëday termination noti...",
          "Type: contracts<br>Text: Features\n\nThe contract lists six core features of the Homellm product, including AI‚Äëpowered risk ass...",
          "Type: contracts<br>Text: Support\n\nSupport provisions include initial training, ongoing technical assistance during business h...",
          "Type: contracts<br>Text: Signatures\n\nThe agreement concludes with signature blocks for both Insurellm and Greenstone Insuranc...",
          "Type: contracts<br>Text: Contract Overview and Key Terms\n\nThe contract, dated April 5, 2025, formalizes a partnership between...",
          "Type: contracts<br>Text: Renewal Terms and Feature Overview\n\nRenewal provisions require a 90‚Äëday notice and guarantee enterpr...",
          "Type: contracts<br>Text: Advanced Features Part I\n\nKey platform capabilities include comprehensive digital health integration...",
          "Type: contracts<br>Text: Advanced Features Part II\n\nThe agreement adds an agent network platform, extensive regulatory compli...",
          "Type: contracts<br>Text: Support Services and Implementation\n\nInsurellm provides extensive enterprise support, including a de...",
          "Type: contracts<br>Text: Signatures and Final Provisions\n\nBoth parties sign the agreement on April 5, 2025, confirming the st...",
          "Type: contracts<br>Text: Contract Overview\n\nThe contract dated January 20, 2025, establishes an agreement between Insurellm, ...",
          "Type: contracts<br>Text: Terms and Renewal\n\nThe Terms section defines the coverage, duration, payment schedule, policy volume...",
          "Type: contracts<br>Text: Renewal Details\n\nThe Renewal clause outlines automatic 12‚Äëmonth extensions unless a 30‚Äëday terminati...",
          "Type: contracts<br>Text: Features Summary\n\nThe Features section lists the Starter Tier capabilities, including AI‚Äëpowered und...",
          "Type: contracts<br>Text: Support and Signatures\n\nThe Support section details onboarding, technical assistance, platform updat...",
          "Type: contracts<br>Text: Contract Overview\n\nThe contract dated March 15, 2025, identifies Insurellm, Inc. and Summit Commerci...",
          "Type: contracts<br>Text: Terms ‚Äì Coverage and Duration\n\nThe agreement outlines that Insurellm will provide Summit Commercial ...",
          "Type: contracts<br>Text: Terms ‚Äì Confidentiality, Liability, Data Security\n\nBoth parties commit to strict confidentiality of ...",
          "Type: contracts<br>Text: Renewal Clause\n\nThe contract will automatically renew for another 18‚Äëmonth term unless a written ter...",
          "Type: contracts<br>Text: Features Overview\n\nSummit Commercial Insurance receives a suite of Bizllm Business Tier features, in...",
          "Type: contracts<br>Text: Support Services\n\nInsurellm will deliver a 3‚Äëweek onboarding program, a dedicated support team, quar...",
          "Type: contracts<br>Text: Signatures\n\nThe agreement is signed by Michael Torres, Chief Revenue Officer of Insurellm, Inc., and...",
          "Type: contracts<br>Text: Contract Overview and Core Terms\n\nThis chunk outlines the contract date, parties, and primary terms ...",
          "Type: contracts<br>Text: Service Level, Confidentiality, and Security\n\nThis chunk details the SLA guarantees, confidentiality...",
          "Type: contracts<br>Text: Renewal Terms and Feature Overview\n\nThis chunk describes the renewal notice period, pricing guarante...",
          "Type: contracts<br>Text: Comprehensive Feature Set Details\n\nThis chunk enumerates the 13 major feature groups ranging from un...",
          "Type: contracts<br>Text: Support Services, Implementation and Signatures\n\nThis chunk covers the extensive support, implementa...",
          "Type: contracts<br>Text: Contract Overview\n\nThe contract titled ‚ÄúContract with TechDrive Insurance for Carllm‚Äù was signed on ...",
          "Type: contracts<br>Text: Terms\n\nThe agreement defines Insurellm as the Provider and TechDrive Insurance as the Customer, gran...",
          "Type: contracts<br>Text: Renewal Terms\n\nThe contract will automatically renew for one‚Äëyear periods unless a party gives writt...",
          "Type: contracts<br>Text: Features Included\n\nUnder the Professional Tier, TechDrive Insurance receives access to AI‚Äëpowered ri...",
          "Type: contracts<br>Text: Support and Training\n\nInsurellm will provide 24/7 AI‚Äëdriven chatbot support and will push regular sy...",
          "Type: contracts<br>Text: Signatures\n\nJohn Smith, Account Manager for Insurellm, and Sarah Johnson, Operations Director for Te...",
          "Type: contracts<br>Text: Contract Terms\n\nThe Terms section defines the contract's 12‚Äëmonth duration, monthly payment obligati...",
          "Type: contracts<br>Text: Renewal Provisions\n\nRenewal provisions state that the agreement will auto‚Äërenew for additional 12‚Äëmo...",
          "Type: contracts<br>Text: Product Features\n\nThe Features section lists the AI‚Äëdriven risk assessment, instant quoting, fraud d...",
          "Type: contracts<br>Text: Support and Training\n\nSupport details provide 24/7 email or chatbot access, regular technical mainte...",
          "Type: contracts<br>Text: Signatures\n\nBoth parties sign the agreement, with John Doe as CEO of Velocity Auto Solutions and Jan...",
          "Type: contracts<br>Text: Contract Terms Overview\n\nThis chunk outlines the effective date, duration, subscription cost, claim ...",
          "Type: contracts<br>Text: Renewal Conditions\n\nThe renewal terms describe automatic renewal, pricing adjustments tied to CPI, a...",
          "Type: contracts<br>Text: Advanced Features Description\n\nDetails the suite of AI-powered claims automation features, including...",
          "Type: contracts<br>Text: Support Services and Signatures\n\nSpecifies technical support, training, maintenance, account managem...",
          "Type: contracts<br>Text: Contract Overview\n\nThis contract, dated March 20, 2025, is identified as CR-2025-E-0078. It establis...",
          "Type: contracts<br>Text: Key Terms\n\nThe agreement grants DriveSmart enterprise access to the Carllm platform with unlimited l...",
          "Type: contracts<br>Text: Renewal and Additional Terms\n\nConfidentiality and data security obligations persist throughout the c...",
          "Type: contracts<br>Text: Features Overview Part 1\n\nDriveSmart receives the full Carllm Enterprise suite, including unlimited ...",
          "Type: contracts<br>Text: Features Overview Part 2\n\nThe platform includes advanced fraud detection, a customer insights dashbo...",
          "Type: contracts<br>Text: Support Services\n\nInsurellm provides dedicated success teams, 24/7 support with rapid response times...",
          "Type: contracts<br>Text: Signatures & Closing\n\nThe contract is signed by Jennifer Rodriguez, CEO of Insurellm, and Steven Bro...",
          "Type: contracts<br>Text: Contract Overview\n\nThe contract identifies the parties, date, number, and outlines the core coverage...",
          "Type: contracts<br>Text: Financial and Service Terms\n\nThis section details the policy volume, service level guarantees, confi...",
          "Type: contracts<br>Text: Renewal and Features Overview\n\nThe renewal clause specifies a 90‚Äëday mutual notice and preferential ...",
          "Type: contracts<br>Text: Feature Set - Platform Capabilities\n\nThe first five features describe unlimited policy administratio...",
          "Type: contracts<br>Text: Feature Set - IoT and Integration\n\nFeatures six through twelve expand on predictive maintenance, mul...",
          "Type: contracts<br>Text: Support Services\n\nThe support section outlines dedicated success teams, 24/7 premium support respons...",
          "Type: contracts<br>Text: Additional Support and Analytics\n\nAdditional support services cover integration, analytics, IoT inno...",
          "Type: contracts<br>Text: Contract Overview and Core Terms\n\nThe contract establishes the parties, start date, duration, fee st...",
          "Type: contracts<br>Text: Lead Quality, Dispute Resolution, and Renewal\n\nThis section defines lead quality criteria, dispute r...",
          "Type: contracts<br>Text: Platform Features Overview\n\nThe contract details platform capabilities such as AI matching, featured...",
          "Type: contracts<br>Text: Support Services and Signatures\n\nThe agreement outlines support provisions including technical assis...",
          "Type: contracts<br>Text: Contract Overview and Parties\n\nThis section records the contract date, number, and identifies the Pr...",
          "Type: contracts<br>Text: Payment Terms and Member Coverage\n\nThe client agrees to monthly payments of $15,000 over an 18‚Äëmonth...",
          "Type: contracts<br>Text: Confidentiality and Renewal Provisions\n\nConfidentiality obligations extend throughout the contract a...",
          "Type: contracts<br>Text: Key Features Offered\n\nThe contract lists extensive platform capabilities, including AI‚Äëdriven claims...",
          "Type: contracts<br>Text: Support Services and Training\n\nInsurellm provides priority customer support with defined response ti...",
          "Type: contracts<br>Text: Signatures and Closing Statement\n\nThe contract is signed by representatives of both Insurellm, Inc. ...",
          "Type: contracts<br>Text: Contract Overview\n\nThis section introduces the agreement between Insurellm and BrightWay Solutions, ...",
          "Type: contracts<br>Text: Terms ‚Äì Scope, Payment, SLA\n\nThe contract details the scope of services, payment obligations, and se...",
          "Type: contracts<br>Text: Renewal Terms\n\nThe agreement includes an automatic one‚Äëyear renewal unless proper notice is given, a...",
          "Type: contracts<br>Text: Features Overview\n\nBrightWay Solutions gains AI‚Äëpowered matching, real‚Äëtime quotes, analytics, and c...",
          "Type: contracts<br>Text: Support and Signatures\n\nThe contract provides dedicated support hours, optional premium support, and...",
          "Type: contracts<br>Text: Terms\n\nThis section defines the effective date, contract duration, subscription cost, payment schedu...",
          "Type: contracts<br>Text: Renewal\n\nThe renewal clause outlines automatic renewal, price protection limits, and volume discount...",
          "Type: contracts<br>Text: Features\n\nThis section enumerates the professional tier functionalities, industry-specific templates...",
          "Type: contracts<br>Text: Support\n\nThe support section details technical assistance, training programs, maintenance schedules,...",
          "Type: contracts<br>Text: Signatures\n\nThe signatures confirm the agreement between Insurellm and Atlantic Risk Solutions, with...",
          "Type: contracts<br>Text: Contract Overview\n\nThis contract, dated October 6, 2023, is between Insurellm, Inc. and GreenValley ...",
          "Type: contracts<br>Text: Terms ‚Äì Coverage and Duration\n\nInsurellm will provide GreenValley Insurance access to the Homellm pr...",
          "Type: contracts<br>Text: Terms ‚Äì Payment, Confidentiality, Liability\n\nGreenValley Insurance will pay a monthly fee of $10,000...",
          "Type: contracts<br>Text: Renewal Clause\n\nIf no party provides a written termination notice at least 30 days before the contra...",
          "Type: contracts<br>Text: Features ‚Äì Product Capabilities\n\nGreenValley Insurance receives six key features, including AI‚Äëpower...",
          "Type: contracts<br>Text: Support Services\n\nInsurellm will provide comprehensive support, including onboarding training, a 24/...",
          "Type: contracts<br>Text: Signatures\n\nThe contract is signed by the CEO of Insurellm, Inc. and the COO of GreenValley Insuranc...",
          "Type: products<br>Text: Product Summary\n\nBizllm is Insurellm's enterprise-grade commercial insurance platform aimed at moder...",
          "Type: products<br>Text: Multi-Line Underwriting Engine\n\nBizllm's underwriting engine handles diverse commercial insurance pr...",
          "Type: products<br>Text: Business Intelligence Integration\n\nThe platform automatically gathers data from registries, financia...",
          "Type: products<br>Text: Cyber Risk Assessment\n\nBizllm offers specialized cyber insurance capabilities, evaluating digital se...",
          "Type: products<br>Text: Workers' Compensation Optimization\n\nIntegrated tools analyze payroll, industry classifications, clai...",
          "Type: products<br>Text: Commercial Property Evaluation\n\nAdvanced property risk modeling uses location data, building charact...",
          "Type: products<br>Text: Professional Liability Specialization\n\nBizllm delivers industry‚Äëspecific professional liability unde...",
          "Type: products<br>Text: Portfolio Management Dashboard\n\nThe dashboard offers analytics on line‚Äëof‚Äëbusiness performance, loss...",
          "Type: products<br>Text: Agent and Broker Portal\n\nDedicated portals let distribution partners quote, bind, and service commer...",
          "Type: products<br>Text: Claims Management Integration\n\nBizllm connects seamlessly with claims systems for fast incident repo...",
          "Type: products<br>Text: Pricing\n\nBizllm offers three tiered pricing plans based on insurer size and required capabilities. A...",
          "Type: products<br>Text: Roadmap\n\nBizllm's roadmap outlines feature releases from version 1.0 in Q2‚ÄØ2025 through internationa...",
          "Type: products<br>Text: Product Overview\n\nRellm is an AI-powered enterprise reinsurance solution designed to transform risk ...",
          "Type: products<br>Text: Key Features\n\nRellm provides AI-driven analytics, seamless system integrations, and a comprehensive ...",
          "Type: products<br>Text: Pricing Plans\n\nInsurellm offers three flexible pricing tiers for Rellm, ranging from a basic plan to...",
          "Type: products<br>Text: Roadmap 2025-2026\n\nThe roadmap outlines major product milestones, including a mobile app, AR feature...",
          "Type: products<br>Text: Product Overview\n\nCarllm is an innovative auto insurance product from Insurellm that leverages AI to...",
          "Type: products<br>Text: Key Features\n\nCarllm combines AI‚Äëpowered risk assessment, instant quoting, and customizable plans to...",
          "Type: products<br>Text: Pricing Model\n\nCarllm is sold via a subscription model with three tiers‚ÄîBasic, Professional, and Ent...",
          "Type: products<br>Text: Roadmap 2025-2026\n\nThe roadmap outlines quarterly 2025 enhancements, including expanded data integra...",
          "Type: products<br>Text: Product Overview\n\nHomellm is an innovative home insurance product from Insurellm that uses advanced ...",
          "Type: products<br>Text: Key Features\n\nHomellm offers six core AI‚Äëdriven capabilities, from risk assessment to a customer por...",
          "Type: products<br>Text: Pricing Options\n\nHomellm pricing scales with insurer size and customization needs, offering Basic, S...",
          "Type: products<br>Text: Roadmap & Future\n\nThe Homellm roadmap outlines major releases from version 1.0 in Q1‚ÄØ2024 to ongoing...",
          "Type: products<br>Text: Claimllm Overview\n\nClaimllm is Insurellm's AI-driven claims processing platform that automates the e...",
          "Type: products<br>Text: Intelligent FNOL Processing\n\nThe AI-powered FNOL intake captures claim details via mobile apps, web ...",
          "Type: products<br>Text: Automated Triage and Routing\n\nMachine learning instantly assesses claim severity, complexity, and fr...",
          "Type: products<br>Text: Computer Vision Damage Assessment\n\nAdvanced image recognition analyzes photos and videos of damaged ...",
          "Type: products<br>Text: Predictive Fraud Detection\n\nSophisticated fraud analytics evaluate claims against historical pattern...",
          "Type: products<br>Text: Smart Document Processing\n\nOCR and NLP automatically extract information from medical records, polic...",
          "Type: products<br>Text: Dynamic Reserve Setting\n\nAI‚Äëpowered predictive modeling analyzes claim characteristics and historica...",
          "Type: products<br>Text: Vendor Management Platform\n\nIntegrated tools coordinate with repair shops, medical providers, lawyer...",
          "Type: products<br>Text: Payment Automation\n\nStraight‚Äëthrough payment processing handles approved claims via direct deposit, ...",
          "Type: products<br>Text: Claimant Communication Hub\n\nOmnichannel tools keep claimants informed via text, email, app notificat...",
          "Type: products<br>Text: Analytics and Reporting\n\nDashboards track metrics such as cycle time, loss ratios, settlement patter...",
          "Type: products<br>Text: Pricing Tiers\n\nClaimllm offers Core, Advanced, and Enterprise tiers to suit insurers of varying size...",
          "Type: products<br>Text: Roadmap Overview\n\nThe roadmap outlines major releases from version 1.0 in Q1‚ÄØ2025 to advanced AI‚Äëdri...",
          "Type: products<br>Text: Product Overview\n\nLifellm is an advanced life insurance platform created by Insurellm that modernize...",
          "Type: products<br>Text: AI-Powered Underwriting\n\nThe AI engine evaluates health records, lifestyle data, and demographics to...",
          "Type: products<br>Text: Predictive Risk Modeling\n\nMachine learning models assess mortality risk using extensive datasets suc...",
          "Type: products<br>Text: Digital Health Integration\n\nLifellm connects with wearables and health apps to offer dynamic policie...",
          "Type: products<br>Text: Automated Policy Management\n\nThe platform provides tools for beneficiary management, premium calcula...",
          "Type: products<br>Text: Smart Document Processing\n\nNatural language processing and OCR automatically extract and validate da...",
          "Type: products<br>Text: Customer Portal\n\nA user-friendly portal lets policyholders manage coverage, update beneficiaries, an...",
          "Type: products<br>Text: Compliance Automation\n\nBuilt‚Äëin tools keep policies aligned with state and federal regulations, upda...",
          "Type: products<br>Text: Pricing\n\nLifellm offers tiered pricing to suit insurers of any size, from a starter tier at $3,500/m...",
          "Type: products<br>Text: Roadmap\n\nThe roadmap outlines major releases from version 1.0 in Q2‚ÄØ2025 to international expansion ...",
          "Type: products<br>Text: Healthllm Summary\n\nHealthllm is Insurellm's comprehensive health insurance platform that empowers pr...",
          "Type: products<br>Text: Intelligent Plan Design\n\nThe AI-powered tools help insurers design competitive health plans by analy...",
          "Type: products<br>Text: Real-Time Eligibility Verification\n\nIntegrated eligibility checking provides instant verification of...",
          "Type: products<br>Text: AI-Driven Claims Adjudication\n\nAutomated claims processing uses machine learning to review claims fo...",
          "Type: products<br>Text: Predictive Healthcare Analytics\n\nAdvanced analytics identify high-risk members who would benefit fro...",
          "Type: products<br>Text: Provider Network Management\n\nComprehensive tools manage provider relationships, contract negotiation...",
          "Type: products<br>Text: Member Engagement Platform\n\nA mobile-first portal gives members easy access to coverage information,...",
          "Type: products<br>Text: Medication Management\n\nIntegration with pharmacy benefit managers enables formulary management, prio...",
          "Type: products<br>Text: Regulatory Compliance Engine\n\nThe built-in compliance monitoring ensures adherence to ACA, state man...",
          "Type: products<br>Text: Pricing Overview\n\nHealthllm offers tiered pricing with Essential, Professional, and Enterprise optio...",
          "Type: products<br>Text: Roadmap Overview\n\nThe roadmap outlines key releases from version 1.0 in Q1 2025 to global expansion ...",
          "Type: products<br>Text: Product Overview\n\nMarkellm is a two-sided marketplace that connects consumers with insurance compani...",
          "Type: products<br>Text: Key Features\n\nThe platform provides AI-powered matching, a user-friendly interface, real‚Äëtime quotes...",
          "Type: products<br>Text: Pricing Model\n\nMarkellm offers free membership for consumers plus premium subscription options, whil...",
          "Type: products<br>Text: 2025 Q1 Roadmap\n\nIn Q1 2025, Markellm will launch a mobile app and introduce a referral program to e...",
          "Type: products<br>Text: 2025 Q2 Roadmap\n\nQ2 2025 focuses on expanding product offerings to life and health insurance and par...",
          "Type: products<br>Text: 2025 Q3 Roadmap\n\nDuring Q3 2025, a comprehensive marketing campaign and the release of testimonials ...",
          "Type: products<br>Text: 2026 Q4 Roadmap\n\nQ4 2026 will implement machine learning enhancements to improve match precision and...",
          "Type: products<br>Text: Conclusion\n\nMarkellm aims to improve the insurance experience through technology and insights, aspir...",
          "Type: employees<br>Text: Personal Summary\n\nPriya Sharma was born on January 8, 1986 and currently works as a Senior Data Scie...",
          "Type: employees<br>Text: Career Progression\n\nPriya has been a Senior Data Scientist at Insurellm since March 2018, leading ML...",
          "Type: employees<br>Text: Annual Performance History\n\nPriya consistently receives high performance ratings, reaching 4.9/5 in ...",
          "Type: employees<br>Text: Compensation History\n\nPriya's compensation has grown steadily, reaching a base salary of $145,000 wi...",
          "Type: employees<br>Text: Other HR Notes\n\nPriya holds a PhD from Stanford, has authored over seven peer‚Äëreviewed papers, and i...",
          "Type: employees<br>Text: Personal Summary\n\nKevin Zhang was born on March 27, 1990 and works as a Mobile Developer in San Fran...",
          "Type: employees<br>Text: Career Progression\n\nSince May 2020 Kevin has led iOS development for Insurellm's consumer mobile app...",
          "Type: employees<br>Text: Annual Performance History\n\nKevin received a 4.6/5 rating in 2023 for launching the Android app on t...",
          "Type: employees<br>Text: Compensation History\n\nKevin's base salary increased from $105,000 in 2020 to $128,000 in 2023, accom...",
          "Type: employees<br>Text: Additional HR Notes\n\nKevin holds a BS in Computer Science from UC Berkeley and is recognized for his...",
          "Type: employees<br>Text: Employee Summary\n\nSamuel Trenton, born April 12, 1989, holds the position of Senior Data Scientist i...",
          "Type: employees<br>Text: Career Progression\n\nSamuel advanced from Junior Data Analyst in 2016 to Senior Data Scientist by 202...",
          "Type: employees<br>Text: Performance History\n\nFrom 2020 to 2023 Samuel's performance ratings ranged from 3.0 to 4.5, with not...",
          "Type: employees<br>Text: Compensation & Additional HR Notes\n\nSamuel's compensation grew from $100,000 base salary in 2020 to ...",
          "Type: employees<br>Text: Employee Summary\n\nMarcus Johnson is a 35-year-old Customer Success Manager based in New York, earnin...",
          "Type: employees<br>Text: Career Progression\n\nSince April 2020, Marcus has served as a Customer Success Manager, overseeing 25...",
          "Type: employees<br>Text: Annual Performance History\n\nMarcus received a 4.7/5 performance rating in 2023, highlighted by excep...",
          "Type: employees<br>Text: Compensation History\n\nHis compensation has risen from a $65,000 base salary in 2018 to $98,000 in 20...",
          "Type: employees<br>Text: Additional HR Notes\n\nMarcus holds a BA in Business Administration from NYU Stern and certifications ...",
          "Type: employees<br>Text: Employee Profile Summary\n\nSamantha Greene is an HR Generalist based in Denver, born on October 14, 1...",
          "Type: employees<br>Text: Career Progression at Insurellm\n\nFrom 2020 to 2023 Samantha advanced from HR Coordinator to a pivota...",
          "Type: employees<br>Text: Performance History 2020-2022\n\nPerformance ratings varied, with an outstanding 2020, a solid 2021, a...",
          "Type: employees<br>Text: Performance & Compensation 2023\n\nIn 2023 Samantha met expectations after conflict‚Äëresolution trainin...",
          "Type: employees<br>Text: Additional HR Contributions\n\nSamantha is pursuing SHRM‚ÄëCP certification, participates in wellness pr...",
          "Type: employees<br>Text: HR Record Overview\n\nThe document provides Alex Thomson‚Äôs basic personal details and outlines his car...",
          "Type: employees<br>Text: Performance and Compensation\n\nAlex Thomson‚Äôs performance history shows consistently high ratings, wi...",
          "Type: employees<br>Text: Additional HR Notes\n\nAdditional HR notes emphasize Alex‚Äôs involvement in diversity initiatives, adva...",
          "Type: employees<br>Text: HR Overview\n\nJennifer Adams was born on July 7, 1997 and works as a remote Sales Development Represe...",
          "Type: employees<br>Text: Career Progression\n\nSince March 2023, Jennifer has been a Sales Development Representative generatin...",
          "Type: employees<br>Text: Performance & Compensation\n\nIn 2023 she received a performance rating of 3.0/5, meeting basic expect...",
          "Type: employees<br>Text: Additional HR Notes\n\nJennifer holds a BA in Business Administration from the University of Denver an...",
          "Type: employees<br>Text: Personal Details & Early Career\n\nJordan Blake was born on March 15, 1993 and works as a Sales Develo...",
          "Type: employees<br>Text: Mid Career Achievements & Performance\n\nJordan was recognized as SDR of the Month for three consecuti...",
          "Type: employees<br>Text: Recent Performance, Compensation & HR Notes\n\nIn 2023 Jordan set to exceed annual targets by 30% in Q...",
          "Type: company<br>Text: Founding and Early Growth\n\nInsurellm was founded in 2015 by Avery Lancaster as an insurance tech sta...",
          "Type: company<br>Text: Strategic Restructuring and Lean Operations\n\nBetween 2022 and 2023 the company restructured to prior...",
          "Type: company<br>Text: Product Expansion and Market Reach\n\nPost‚Äërestructuring, Insurellm broadened its suite to eight platf..."
         ],
         "type": "scatter",
         "x": {
          "bdata": "gZP1wOy3isEOHxJBxYHgwHkZskDpYJnAzxxrwZ1zl0DL+rXAgQICwd4kicFejLVAfVoMwDmeq8B9hHLB7tYnQdKgncDXmEdAuJ+BwOsVscBcHpLBe6GYQDe07MA5DpK+QH47wB95jcFBWyVBb2zSwNOOzMBcBsvAkzuHwa7Co0DuFLVAx7CSwEfHY0BFDh7B77ifwblvlEDzI+rAoU2iQAcrw8C1DZPBI1FiQBNcjsD0K/nAOc4GwX6JJkHRF7TA2r62wOsZgsGGBChB2EF5wPIvPUAaIB7B0L+ewWSAGkHv86bAE/uDQHgtFMHkCYjBgxWvQAYtwMAa5ZnAfySPwYVCKEG7bLnAPw7HPzZ8AMAqmwLBzZ6JwUSw7UAD5+jA0ERkQGKXMMEmOpPBSbDXQNLrDcHlNINAq2NCv0QheMFmj1jA/0hlwUfeu0AZ9ZHAn5WIQP4/a8Bv1I7BJSlcQKcr+MBrOKBAle3dwG9P9MANm6JAGk85QAAWdMBdgjpA1G1nwApPB8GOhIHBiVqvQOCursDZJFy/oqbqwD/ktMCmmIHB3RcoQZg7bcBu8ntAmUrBwNi04sByNFLBdeDWQK3whcDpSIo+LHpEv56liME7mblAhbDXvy63jz9IWuC/Q1iEweuQ6kDSgsO/kuRaQKwEv8B4QprB9SMpQbcL8sAaGO4+jcaZwBjOdsEfbrVAcD+5QBE0hMDsUvs/VscMwWaFCMFeKxfB55oYQHeCoD8QyQ3AocmLv8rl88CjhTrBOoEiQZAJv8AyhCvAiMIYwT80a8FJRdBAjei3wDwdTUBVm7S/TDjSwCcYiMHFCSBBZ92BwGShFcGOdZrBzEsaQRWticCQAYVAfDAQwHlkU8D242zARV54wZDUeUByNonAkZ+QQCGh6kDZ2QVBmzdDQY0a1UDubj9BzEixPzTd/0A8eXVBP60LQORdQEFBdeW/1YGkP/hdnUB3369AroEzwP7UwkC1WeVA8fjhQNqPgMB0LAFBdxI7QH8yZ0AvsTTAnWcXQEc1P0FqATtBYspTQY0gLUFckGRBuNdBQVstXEG5M49A5ydQQTEEUD+4DnZBwsMhwOZSLkHk5DRAelcRQRZ6DEEghFlAGG7qQEx1QkG6Zt8+rvL+QLaiIMAHPHdAJaz3QELK9kCiwgFBk4RRQfUlBUGEPYFAHZW2Pmg00UCKJQRBMOgDwPqm5EAN8xBBLvsQQVMjscBmQkhB+T8KQdg8TEFs+BhBMioHQS/x/cAE+e/Ae03cwGoNBMF3+uXASZvVwPqdM8DceC3A3q7swIpdQcDCrsbA7YUEwfxmycA4SPrAz52swH+zlcDe5ZbA0Yf2wE9IlMBVPSPBcdAnwTr3IMGaXxjB5zUxwV6U/cCDlwXB+qADwSE+HcFyLBDBNW4MwaSnFcFDDt7AKzz5wAoj9sA9Y5g/1tLxPjeWyD8=",
          "dtype": "f4"
         },
         "y": {
          "bdata": "77tuwTB9mMHmHS5AwGCOP6oOLcH+gwrBooyDweY26j61oK6/bSJZwYnhecHKxppAnPDdPqLyN8Exf4LBdaweQelUM0B3Wx7BSlviwB3Z7sBjmm/BKJcFQTe5rkDWAs3AXzKbwEZAkcHUUjhAbxhMQGdchsGFDBfBjAhjwfamKkFZvB9B+FeiQFf7C8G+Xi/BpQyGwTNSCEEQ3odAI/gKwR0PVcFMfpPBmhp/QBogg7/wdzrBMg83wcoUmcDNNYtAqU4ewZOZbcEsGAVB47xUQNPWF8Hc/krBcVWQwZgLV8BeBQtAvpXbvwoFZ8GT3IjBdKVDPz1poj1MRznBIxaGwd4UHEGov7A/nG/swAae38CNfxbBOa2CwVEmFkF3GhRA/zECwTq1FMF91H7BKnMWQUMyNUDaXfTAnCrgwIbyi8FWV0rBMKGbwe1gI74yqzE/pmZCwcsKHcGXFIvBwWBPQIMlMT9CujrB5AcJwTET+MDD7RVB144tQbwCcEB0ygzBVaXiwMFhFcHCu4bBdZz8QIOESEBNiwjBXyI2wd3klsC9tpHBVSFuwHa9AEDfQyrB3jQlwU51icBGSUbBDCgCvyXzkUBPky/Bx600wT9rjMHWnItAIeZoP9JBMMHp2S7BF8qAwaDZj0DUUFq9VCc9wRf6OMEZe4rBK9UNQbIRikCSMRfBRohqwXY2VsFxaXVAHb29QA33Y0C19hLB82irwP6BpcCPcqzAZ5KkQAB8uEDUIZdAa3ijQFGhdcHEVoDBhS9NQKxvp0CwOcnAMK8PwVzubcHUrxhBNez+PyY+7cDkKL7A9f9xwWJPkcGJaQ5AqQkIwN3dSsEt04DBVrJZwAaD5D+3+xTBQaEDwfRfGsFiaS3B5f2WweJ/gUBNNK88Sps4wZCpG8CAKuC/P/9/QKQeRcAAwzTAwRiRQFBhh8CtpY9AOhpLQStswUCxy2nBNXctQEqra7/hDLk8gYhbwZr2DED9Pk9AcO52QAqyd8GgnpRAYsDFP/62L0AGkG3B5eobQPcn9EDsoyNBdroKQd0rQkE7KwJBny5YQYeS0EDyYmBBSjMlQfFN7EBDUJFA7SF5wfZr7kD4HY8++LmNv8ZY8UBQQ+hAC/JmQeXaWEGvSy5B0GRqQSC3a8GTAM1ATVDeQI3f0kB0QCtBH7wAQZasBkFbZ2BBpYotQZh+OkFajWtBYUp0weRp+UAue5c/ucTcP9jAhcEn66I/pBqjQFy4qD+sa6FA4MBlP3gVtEHXNmFBEcNuQSgTrkGvFF5Bif+1QYy0X0FigmNBFEGpQanpV0HxRaxBncJhQSaad0GxK6FBFoOgQRH+hkGfaYVB/YmoQSEsjEHmwqRBcseSQV2AmEGWSp9Baq+RQY+ujEFYyJNBJk+QQc5Nq0EX84BBJTSgQev0gEHtEJFBFE6FQQ+MmkEdDk/AZN5VwAnqOsA=",
          "dtype": "f4"
         }
        }
       ],
       "layout": {
        "height": 600,
        "margin": {
         "b": 10,
         "l": 10,
         "r": 20,
         "t": 40
        },
        "scene": {
         "xaxis": {
          "title": {
           "text": "x"
          }
         },
         "yaxis": {
          "title": {
           "text": "y"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "2D Chroma Vector Store Visualization"
        },
        "width": 800
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(title='2D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "fd0fc23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "text",
         "marker": {
          "color": [
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "red",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "green",
           "orange",
           "orange",
           "orange"
          ],
          "opacity": 0.8,
          "size": 5
         },
         "mode": "markers",
         "text": [
          "Type: contracts<br>Text: Contract Terms\n\nThis section establishes the parties and outlines the core service provisions, durat...",
          "Type: contracts<br>Text: Renewal Terms\n\nThe renewal clause details how the agreement may be extended for additional one‚Äëyear ...",
          "Type: contracts<br>Text: Platform Features\n\nThis section enumerates the AI‚Äëdriven matching, real‚Äëtime quoting, analytics, opt...",
          "Type: contracts<br>Text: Support Services\n\nThe support provisions define technical support hours, response time commitments, ...",
          "Type: contracts<br>Text: Agreement Acceptance\n\nThe acceptance section provides signature lines for both parties, confirming t...",
          "Type: contracts<br>Text: Terms and Parties\n\nThe agreement defines the parties, product description, payment terms, and usage ...",
          "Type: contracts<br>Text: Renewal Terms\n\nThe contract automatically renews yearly unless a 60‚Äëday termination notice is given....",
          "Type: contracts<br>Text: Key Features\n\nRellm offers AI‚Äëdriven analytics, a customizable dashboard, compliance tools, and clie...",
          "Type: contracts<br>Text: Support and Services\n\nInsurellm provides 24/7 customer support, initial training, free updates, and ...",
          "Type: contracts<br>Text: Terms\n\nThe contract becomes effective on January 1, 2025 and runs for a 12‚Äëmonth term ending Decembe...",
          "Type: contracts<br>Text: Renewal\n\nThe agreement will auto‚Äërenew for another 12‚Äëmonth period unless either party gives a 30‚Äëda...",
          "Type: contracts<br>Text: Features\n\nRoadway Insurance Inc. gains access to all Professional Tier capabilities, including AI‚Äëpo...",
          "Type: contracts<br>Text: Support\n\nThe contract provides priority technical support, up to five training sessions, and quarter...",
          "Type: contracts<br>Text: Terms of Agreement\n\nThe contract outlines coverage, duration, payment, and overage fees for the Clai...",
          "Type: contracts<br>Text: Renewal Provisions\n\nThe agreement automatically renews for successive 12‚Äëmonth terms unless a 30‚Äëday...",
          "Type: contracts<br>Text: Core Tier Features\n\nRapid Claims Associates receives a suite of AI‚Äëdriven claim processing tools, in...",
          "Type: contracts<br>Text: Support Services\n\nInsurellm provides onboarding, technical support, regular platform updates, and de...",
          "Type: contracts<br>Text: Signatures & Final Clause\n\nThe contract is signed by senior representatives of Insurellm and Rapid C...",
          "Type: contracts<br>Text: Contract Overview\n\nThis contract, dated February 12, 2025, establishes an agreement between Insurell...",
          "Type: contracts<br>Text: Terms\n\nThe Terms section defines the parties, scope of services, payment schedule, policy volume, an...",
          "Type: contracts<br>Text: Renewal\n\nThe Renewal clause specifies automatic 12‚Äëmonth renewals unless a 45‚Äëday notice is given, a...",
          "Type: contracts<br>Text: Features\n\nThe Features section details AI‚Äëpowered underwriting, predictive risk modeling, digital he...",
          "Type: contracts<br>Text: Support\n\nSupport provisions include priority technical assistance, a comprehensive training program,...",
          "Type: contracts<br>Text: Contract Overview\n\nThe contract between Insurellm and GreenField Holdings becomes effective on Novem...",
          "Type: contracts<br>Text: Terms of Agreement\n\nThe agreement defines Insurellm as the Provider and GreenField Holdings as the C...",
          "Type: contracts<br>Text: Renewal Provisions\n\nThe contract will automatically renew for one‚Äëyear terms unless a written non‚Äëre...",
          "Type: contracts<br>Text: Platform Features\n\nThe contract lists four core features of the Markellm platform, from AI‚Äëpowered m...",
          "Type: contracts<br>Text: Support and Training\n\nInsurellm will provide the client with phone and email support, onboarding tra...",
          "Type: contracts<br>Text: Pricing and Signatures\n\nPricing consists of a fixed monthly listing fee and a performance‚Äëbased char...",
          "Type: contracts<br>Text: Contract Overview and Core Terms\n\nThis section outlines the contract date, number, parties, and the ...",
          "Type: contracts<br>Text: Renewal Terms and Pricing Adjustments\n\nThe renewal clause defines a 120‚Äëday notice period, most‚Äëfavo...",
          "Type: contracts<br>Text: Key Platform Features ‚Äì Member, Multi‚ÄëState, Branding, AI Claims\n\nThis chunk details the primary cap...",
          "Type: contracts<br>Text: Extended Feature Set ‚Äì Health Management, APIs, Network, Medication, Engagement, Quality, Analytics,...",
          "Type: contracts<br>Text: Comprehensive Support Services\n\nThe support chapter defines a dedicated success team, 24/7 premium s...",
          "Type: contracts<br>Text: Signatures and Final Provisions\n\nThe concluding section records the authorized signatures of both pa...",
          "Type: contracts<br>Text: Terms\n\nThe contract becomes effective on March‚ÄØ1‚ÄØ2025 and runs for 24 months until February‚ÄØ28‚ÄØ2027....",
          "Type: contracts<br>Text: Renewal\n\nThe agreement auto‚Äërenews for another 24‚Äëmonth term unless a 60‚Äëday notice is given, with p...",
          "Type: contracts<br>Text: Features\n\nGuardian gains full access to Growth Tier capabilities, including AI‚Äëdriven underwriting, ...",
          "Type: contracts<br>Text: Support\n\nComprehensive support includes priority technical assistance, extensive training programs, ...",
          "Type: contracts<br>Text: Signatures\n\nThe contract is signed by Michael Torres, CRO of Insurellm, Inc., and Jonathan Park, Pre...",
          "Type: contracts<br>Text: Terms\n\nThis section defines the parties, license grant, payment obligations, and the initial two‚Äëyea...",
          "Type: contracts<br>Text: Renewal\n\nThe contract automatically renews for one‚Äëyear periods unless notice is given, and subscrip...",
          "Type: contracts<br>Text: Features\n\nThe product provides AI‚Äëdriven risk assessment, dynamic pricing, rapid claim processing, p...",
          "Type: contracts<br>Text: Support\n\nInsurellm commits to 24/7 technical support, onsite and webinar training, and regular syste...",
          "Type: contracts<br>Text: Contract Overview and Core Terms\n\nThis chunk outlines the contract date, parties, and key terms such...",
          "Type: contracts<br>Text: Liability, SLA, Exclusivity and Renewal Provisions\n\nThis chunk details liability limits, SLA uptime ...",
          "Type: contracts<br>Text: Feature Set Delivered to Continental Commercial Group\n\nThis chunk enumerates the comprehensive Bizll...",
          "Type: contracts<br>Text: Support Services, Signatures and Final Agreement\n\nThis chunk outlines the extensive support commitme...",
          "Type: contracts<br>Text: Contract Overview\n\nThe contract, dated April 20, 2025, establishes the agreement between Insurellm, ...",
          "Type: contracts<br>Text: Renewal Terms\n\nThe agreement includes a 120‚Äëday mutual renewal notice and guarantees that National C...",
          "Type: contracts<br>Text: Features Overview\n\nNational Claims Network receives the full Claimllm Enterprise suite, including un...",
          "Type: contracts<br>Text: Support Services\n\nInsurellm provides extensive enterprise‚Äëlevel support covering dedicated success t...",
          "Type: contracts<br>Text: Signatures and Closing\n\nThe contract is signed by Jennifer Rodriguez of Insurellm and Amanda Richard...",
          "Type: contracts<br>Text: Terms Overview\n\nThe contract becomes effective on February‚ÄØ1‚ÄØ2025 and runs for a 24‚Äëmonth term endin...",
          "Type: contracts<br>Text: Renewal Terms\n\nThe agreement automatically renews for another 24‚Äëmonth period unless a 60‚Äëday writte...",
          "Type: contracts<br>Text: Feature Set\n\nThe Professional Tier grants Fortress Business Underwriters access to a comprehensive s...",
          "Type: contracts<br>Text: Support and Maintenance\n\nInsurellm provides priority technical support, dedicated account management...",
          "Type: contracts<br>Text: Contract Overview and Parties\n\nThe agreement establishes Insurellm, Inc. as the Provider and Apex Re...",
          "Type: contracts<br>Text: Payment Terms and Contract Duration\n\nThe Client agrees to pay $10,000 monthly, with payments due on ...",
          "Type: contracts<br>Text: Renewal Provisions\n\nThe agreement automatically renews for one-year terms unless a thirty‚Äëday termin...",
          "Type: contracts<br>Text: Key Solution Features\n\nThe Rellm platform offers AI‚Äëdriven analytics, seamless system integrations, ...",
          "Type: contracts<br>Text: Support Services and Acceptance\n\nProvider delivers technical support, training for up to ten staff m...",
          "Type: contracts<br>Text: Terms Overview\n\nThe contract defines the parties, a non-exclusive license, payment schedule, claims ...",
          "Type: contracts<br>Text: Renewal Terms\n\nThe agreement automatically renews for successive 12‚Äëmonth periods unless a 45‚Äëday no...",
          "Type: contracts<br>Text: Features Summary\n\nFastTrack receives access to a comprehensive suite of AI‚Äëdriven claim processing f...",
          "Type: contracts<br>Text: Support Services\n\nInsurellm will provide priority customer support with defined response times and 2...",
          "Type: contracts<br>Text: Signatures\n\nThe contract is signed by Sarah Chen, VP of Sales for Insurellm, and Rebecca Martinez, C...",
          "Type: contracts<br>Text: Contract Overview\n\nThe contract establishes the agreement between Insurellm, Inc. and WellCare Insur...",
          "Type: contracts<br>Text: Terms Summary\n\nThe Terms section defines coverage, duration, payment, member limits, confidentiality...",
          "Type: contracts<br>Text: Renewal and Pricing\n\nThe Renewal clause states the agreement auto‚Äërenews for successive 12‚Äëmonth per...",
          "Type: contracts<br>Text: Features Overview\n\nThe Features section lists the Essential Tier capabilities, including intelligent...",
          "Type: contracts<br>Text: Support Services\n\nThe Support section describes onboarding, technical assistance, and service hours,...",
          "Type: contracts<br>Text: Signatures and Conclusion\n\nThe signatures confirm the agreement by senior executives of both compani...",
          "Type: contracts<br>Text: Terms\n\nThe contract becomes effective on Jan 25, 2025 and runs for 24 months ending Jan 24, 2027. It...",
          "Type: contracts<br>Text: Renewal\n\nThe agreement auto‚Äërenews for another 24‚Äëmonth term unless a 90‚Äëday notice is given. Price ...",
          "Type: contracts<br>Text: Features\n\nHarmony gains access to the Professional Tier‚Äôs AI‚Äëdriven tools, including plan design, el...",
          "Type: contracts<br>Text: Support Services\n\nHealthllm provides priority technical support, extensive training, regular updates...",
          "Type: contracts<br>Text: Signatures\n\nThe contract is signed by Sarah Chen of Insurellm, Inc. and Dr. Karen Phillips of Harmon...",
          "Type: contracts<br>Text: Contract Overview\n\nThe contract is established between Insurellm and Stellar Insurance Co., specifyi...",
          "Type: contracts<br>Text: Duration and Renewal\n\nThe agreement starts with a 12‚Äëmonth term and automatically renews for success...",
          "Type: contracts<br>Text: Payment Terms\n\nStellar Insurance Co. will pay a monthly subscription of $10,000 for the Professional...",
          "Type: contracts<br>Text: Termination\n\nEither party can end the contract with a 30‚Äëday written notice. A material breach allow...",
          "Type: contracts<br>Text: Features\n\nStellar Insurance Co. receives multiple Rellm features, including AI‚Äëdriven analytics, sea...",
          "Type: contracts<br>Text: Support Services\n\nInsurellm provides 24/7 technical support, quarterly account reviews, training ses...",
          "Type: contracts<br>Text: Signatures\n\nBoth parties acknowledge receipt of the product summary and agree to the contract terms....",
          "Type: contracts<br>Text: Terms\n\nThis section defines the parties, the services provided, contract duration, payment terms, an...",
          "Type: contracts<br>Text: Renewal\n\nThe renewal clause specifies automatic one-year extensions unless a 60‚Äëday termination noti...",
          "Type: contracts<br>Text: Features\n\nThe contract lists six core features of the Homellm product, including AI‚Äëpowered risk ass...",
          "Type: contracts<br>Text: Support\n\nSupport provisions include initial training, ongoing technical assistance during business h...",
          "Type: contracts<br>Text: Signatures\n\nThe agreement concludes with signature blocks for both Insurellm and Greenstone Insuranc...",
          "Type: contracts<br>Text: Contract Overview and Key Terms\n\nThe contract, dated April 5, 2025, formalizes a partnership between...",
          "Type: contracts<br>Text: Renewal Terms and Feature Overview\n\nRenewal provisions require a 90‚Äëday notice and guarantee enterpr...",
          "Type: contracts<br>Text: Advanced Features Part I\n\nKey platform capabilities include comprehensive digital health integration...",
          "Type: contracts<br>Text: Advanced Features Part II\n\nThe agreement adds an agent network platform, extensive regulatory compli...",
          "Type: contracts<br>Text: Support Services and Implementation\n\nInsurellm provides extensive enterprise support, including a de...",
          "Type: contracts<br>Text: Signatures and Final Provisions\n\nBoth parties sign the agreement on April 5, 2025, confirming the st...",
          "Type: contracts<br>Text: Contract Overview\n\nThe contract dated January 20, 2025, establishes an agreement between Insurellm, ...",
          "Type: contracts<br>Text: Terms and Renewal\n\nThe Terms section defines the coverage, duration, payment schedule, policy volume...",
          "Type: contracts<br>Text: Renewal Details\n\nThe Renewal clause outlines automatic 12‚Äëmonth extensions unless a 30‚Äëday terminati...",
          "Type: contracts<br>Text: Features Summary\n\nThe Features section lists the Starter Tier capabilities, including AI‚Äëpowered und...",
          "Type: contracts<br>Text: Support and Signatures\n\nThe Support section details onboarding, technical assistance, platform updat...",
          "Type: contracts<br>Text: Contract Overview\n\nThe contract dated March 15, 2025, identifies Insurellm, Inc. and Summit Commerci...",
          "Type: contracts<br>Text: Terms ‚Äì Coverage and Duration\n\nThe agreement outlines that Insurellm will provide Summit Commercial ...",
          "Type: contracts<br>Text: Terms ‚Äì Confidentiality, Liability, Data Security\n\nBoth parties commit to strict confidentiality of ...",
          "Type: contracts<br>Text: Renewal Clause\n\nThe contract will automatically renew for another 18‚Äëmonth term unless a written ter...",
          "Type: contracts<br>Text: Features Overview\n\nSummit Commercial Insurance receives a suite of Bizllm Business Tier features, in...",
          "Type: contracts<br>Text: Support Services\n\nInsurellm will deliver a 3‚Äëweek onboarding program, a dedicated support team, quar...",
          "Type: contracts<br>Text: Signatures\n\nThe agreement is signed by Michael Torres, Chief Revenue Officer of Insurellm, Inc., and...",
          "Type: contracts<br>Text: Contract Overview and Core Terms\n\nThis chunk outlines the contract date, parties, and primary terms ...",
          "Type: contracts<br>Text: Service Level, Confidentiality, and Security\n\nThis chunk details the SLA guarantees, confidentiality...",
          "Type: contracts<br>Text: Renewal Terms and Feature Overview\n\nThis chunk describes the renewal notice period, pricing guarante...",
          "Type: contracts<br>Text: Comprehensive Feature Set Details\n\nThis chunk enumerates the 13 major feature groups ranging from un...",
          "Type: contracts<br>Text: Support Services, Implementation and Signatures\n\nThis chunk covers the extensive support, implementa...",
          "Type: contracts<br>Text: Contract Overview\n\nThe contract titled ‚ÄúContract with TechDrive Insurance for Carllm‚Äù was signed on ...",
          "Type: contracts<br>Text: Terms\n\nThe agreement defines Insurellm as the Provider and TechDrive Insurance as the Customer, gran...",
          "Type: contracts<br>Text: Renewal Terms\n\nThe contract will automatically renew for one‚Äëyear periods unless a party gives writt...",
          "Type: contracts<br>Text: Features Included\n\nUnder the Professional Tier, TechDrive Insurance receives access to AI‚Äëpowered ri...",
          "Type: contracts<br>Text: Support and Training\n\nInsurellm will provide 24/7 AI‚Äëdriven chatbot support and will push regular sy...",
          "Type: contracts<br>Text: Signatures\n\nJohn Smith, Account Manager for Insurellm, and Sarah Johnson, Operations Director for Te...",
          "Type: contracts<br>Text: Contract Terms\n\nThe Terms section defines the contract's 12‚Äëmonth duration, monthly payment obligati...",
          "Type: contracts<br>Text: Renewal Provisions\n\nRenewal provisions state that the agreement will auto‚Äërenew for additional 12‚Äëmo...",
          "Type: contracts<br>Text: Product Features\n\nThe Features section lists the AI‚Äëdriven risk assessment, instant quoting, fraud d...",
          "Type: contracts<br>Text: Support and Training\n\nSupport details provide 24/7 email or chatbot access, regular technical mainte...",
          "Type: contracts<br>Text: Signatures\n\nBoth parties sign the agreement, with John Doe as CEO of Velocity Auto Solutions and Jan...",
          "Type: contracts<br>Text: Contract Terms Overview\n\nThis chunk outlines the effective date, duration, subscription cost, claim ...",
          "Type: contracts<br>Text: Renewal Conditions\n\nThe renewal terms describe automatic renewal, pricing adjustments tied to CPI, a...",
          "Type: contracts<br>Text: Advanced Features Description\n\nDetails the suite of AI-powered claims automation features, including...",
          "Type: contracts<br>Text: Support Services and Signatures\n\nSpecifies technical support, training, maintenance, account managem...",
          "Type: contracts<br>Text: Contract Overview\n\nThis contract, dated March 20, 2025, is identified as CR-2025-E-0078. It establis...",
          "Type: contracts<br>Text: Key Terms\n\nThe agreement grants DriveSmart enterprise access to the Carllm platform with unlimited l...",
          "Type: contracts<br>Text: Renewal and Additional Terms\n\nConfidentiality and data security obligations persist throughout the c...",
          "Type: contracts<br>Text: Features Overview Part 1\n\nDriveSmart receives the full Carllm Enterprise suite, including unlimited ...",
          "Type: contracts<br>Text: Features Overview Part 2\n\nThe platform includes advanced fraud detection, a customer insights dashbo...",
          "Type: contracts<br>Text: Support Services\n\nInsurellm provides dedicated success teams, 24/7 support with rapid response times...",
          "Type: contracts<br>Text: Signatures & Closing\n\nThe contract is signed by Jennifer Rodriguez, CEO of Insurellm, and Steven Bro...",
          "Type: contracts<br>Text: Contract Overview\n\nThe contract identifies the parties, date, number, and outlines the core coverage...",
          "Type: contracts<br>Text: Financial and Service Terms\n\nThis section details the policy volume, service level guarantees, confi...",
          "Type: contracts<br>Text: Renewal and Features Overview\n\nThe renewal clause specifies a 90‚Äëday mutual notice and preferential ...",
          "Type: contracts<br>Text: Feature Set - Platform Capabilities\n\nThe first five features describe unlimited policy administratio...",
          "Type: contracts<br>Text: Feature Set - IoT and Integration\n\nFeatures six through twelve expand on predictive maintenance, mul...",
          "Type: contracts<br>Text: Support Services\n\nThe support section outlines dedicated success teams, 24/7 premium support respons...",
          "Type: contracts<br>Text: Additional Support and Analytics\n\nAdditional support services cover integration, analytics, IoT inno...",
          "Type: contracts<br>Text: Contract Overview and Core Terms\n\nThe contract establishes the parties, start date, duration, fee st...",
          "Type: contracts<br>Text: Lead Quality, Dispute Resolution, and Renewal\n\nThis section defines lead quality criteria, dispute r...",
          "Type: contracts<br>Text: Platform Features Overview\n\nThe contract details platform capabilities such as AI matching, featured...",
          "Type: contracts<br>Text: Support Services and Signatures\n\nThe agreement outlines support provisions including technical assis...",
          "Type: contracts<br>Text: Contract Overview and Parties\n\nThis section records the contract date, number, and identifies the Pr...",
          "Type: contracts<br>Text: Payment Terms and Member Coverage\n\nThe client agrees to monthly payments of $15,000 over an 18‚Äëmonth...",
          "Type: contracts<br>Text: Confidentiality and Renewal Provisions\n\nConfidentiality obligations extend throughout the contract a...",
          "Type: contracts<br>Text: Key Features Offered\n\nThe contract lists extensive platform capabilities, including AI‚Äëdriven claims...",
          "Type: contracts<br>Text: Support Services and Training\n\nInsurellm provides priority customer support with defined response ti...",
          "Type: contracts<br>Text: Signatures and Closing Statement\n\nThe contract is signed by representatives of both Insurellm, Inc. ...",
          "Type: contracts<br>Text: Contract Overview\n\nThis section introduces the agreement between Insurellm and BrightWay Solutions, ...",
          "Type: contracts<br>Text: Terms ‚Äì Scope, Payment, SLA\n\nThe contract details the scope of services, payment obligations, and se...",
          "Type: contracts<br>Text: Renewal Terms\n\nThe agreement includes an automatic one‚Äëyear renewal unless proper notice is given, a...",
          "Type: contracts<br>Text: Features Overview\n\nBrightWay Solutions gains AI‚Äëpowered matching, real‚Äëtime quotes, analytics, and c...",
          "Type: contracts<br>Text: Support and Signatures\n\nThe contract provides dedicated support hours, optional premium support, and...",
          "Type: contracts<br>Text: Terms\n\nThis section defines the effective date, contract duration, subscription cost, payment schedu...",
          "Type: contracts<br>Text: Renewal\n\nThe renewal clause outlines automatic renewal, price protection limits, and volume discount...",
          "Type: contracts<br>Text: Features\n\nThis section enumerates the professional tier functionalities, industry-specific templates...",
          "Type: contracts<br>Text: Support\n\nThe support section details technical assistance, training programs, maintenance schedules,...",
          "Type: contracts<br>Text: Signatures\n\nThe signatures confirm the agreement between Insurellm and Atlantic Risk Solutions, with...",
          "Type: contracts<br>Text: Contract Overview\n\nThis contract, dated October 6, 2023, is between Insurellm, Inc. and GreenValley ...",
          "Type: contracts<br>Text: Terms ‚Äì Coverage and Duration\n\nInsurellm will provide GreenValley Insurance access to the Homellm pr...",
          "Type: contracts<br>Text: Terms ‚Äì Payment, Confidentiality, Liability\n\nGreenValley Insurance will pay a monthly fee of $10,000...",
          "Type: contracts<br>Text: Renewal Clause\n\nIf no party provides a written termination notice at least 30 days before the contra...",
          "Type: contracts<br>Text: Features ‚Äì Product Capabilities\n\nGreenValley Insurance receives six key features, including AI‚Äëpower...",
          "Type: contracts<br>Text: Support Services\n\nInsurellm will provide comprehensive support, including onboarding training, a 24/...",
          "Type: contracts<br>Text: Signatures\n\nThe contract is signed by the CEO of Insurellm, Inc. and the COO of GreenValley Insuranc...",
          "Type: products<br>Text: Product Summary\n\nBizllm is Insurellm's enterprise-grade commercial insurance platform aimed at moder...",
          "Type: products<br>Text: Multi-Line Underwriting Engine\n\nBizllm's underwriting engine handles diverse commercial insurance pr...",
          "Type: products<br>Text: Business Intelligence Integration\n\nThe platform automatically gathers data from registries, financia...",
          "Type: products<br>Text: Cyber Risk Assessment\n\nBizllm offers specialized cyber insurance capabilities, evaluating digital se...",
          "Type: products<br>Text: Workers' Compensation Optimization\n\nIntegrated tools analyze payroll, industry classifications, clai...",
          "Type: products<br>Text: Commercial Property Evaluation\n\nAdvanced property risk modeling uses location data, building charact...",
          "Type: products<br>Text: Professional Liability Specialization\n\nBizllm delivers industry‚Äëspecific professional liability unde...",
          "Type: products<br>Text: Portfolio Management Dashboard\n\nThe dashboard offers analytics on line‚Äëof‚Äëbusiness performance, loss...",
          "Type: products<br>Text: Agent and Broker Portal\n\nDedicated portals let distribution partners quote, bind, and service commer...",
          "Type: products<br>Text: Claims Management Integration\n\nBizllm connects seamlessly with claims systems for fast incident repo...",
          "Type: products<br>Text: Pricing\n\nBizllm offers three tiered pricing plans based on insurer size and required capabilities. A...",
          "Type: products<br>Text: Roadmap\n\nBizllm's roadmap outlines feature releases from version 1.0 in Q2‚ÄØ2025 through internationa...",
          "Type: products<br>Text: Product Overview\n\nRellm is an AI-powered enterprise reinsurance solution designed to transform risk ...",
          "Type: products<br>Text: Key Features\n\nRellm provides AI-driven analytics, seamless system integrations, and a comprehensive ...",
          "Type: products<br>Text: Pricing Plans\n\nInsurellm offers three flexible pricing tiers for Rellm, ranging from a basic plan to...",
          "Type: products<br>Text: Roadmap 2025-2026\n\nThe roadmap outlines major product milestones, including a mobile app, AR feature...",
          "Type: products<br>Text: Product Overview\n\nCarllm is an innovative auto insurance product from Insurellm that leverages AI to...",
          "Type: products<br>Text: Key Features\n\nCarllm combines AI‚Äëpowered risk assessment, instant quoting, and customizable plans to...",
          "Type: products<br>Text: Pricing Model\n\nCarllm is sold via a subscription model with three tiers‚ÄîBasic, Professional, and Ent...",
          "Type: products<br>Text: Roadmap 2025-2026\n\nThe roadmap outlines quarterly 2025 enhancements, including expanded data integra...",
          "Type: products<br>Text: Product Overview\n\nHomellm is an innovative home insurance product from Insurellm that uses advanced ...",
          "Type: products<br>Text: Key Features\n\nHomellm offers six core AI‚Äëdriven capabilities, from risk assessment to a customer por...",
          "Type: products<br>Text: Pricing Options\n\nHomellm pricing scales with insurer size and customization needs, offering Basic, S...",
          "Type: products<br>Text: Roadmap & Future\n\nThe Homellm roadmap outlines major releases from version 1.0 in Q1‚ÄØ2024 to ongoing...",
          "Type: products<br>Text: Claimllm Overview\n\nClaimllm is Insurellm's AI-driven claims processing platform that automates the e...",
          "Type: products<br>Text: Intelligent FNOL Processing\n\nThe AI-powered FNOL intake captures claim details via mobile apps, web ...",
          "Type: products<br>Text: Automated Triage and Routing\n\nMachine learning instantly assesses claim severity, complexity, and fr...",
          "Type: products<br>Text: Computer Vision Damage Assessment\n\nAdvanced image recognition analyzes photos and videos of damaged ...",
          "Type: products<br>Text: Predictive Fraud Detection\n\nSophisticated fraud analytics evaluate claims against historical pattern...",
          "Type: products<br>Text: Smart Document Processing\n\nOCR and NLP automatically extract information from medical records, polic...",
          "Type: products<br>Text: Dynamic Reserve Setting\n\nAI‚Äëpowered predictive modeling analyzes claim characteristics and historica...",
          "Type: products<br>Text: Vendor Management Platform\n\nIntegrated tools coordinate with repair shops, medical providers, lawyer...",
          "Type: products<br>Text: Payment Automation\n\nStraight‚Äëthrough payment processing handles approved claims via direct deposit, ...",
          "Type: products<br>Text: Claimant Communication Hub\n\nOmnichannel tools keep claimants informed via text, email, app notificat...",
          "Type: products<br>Text: Analytics and Reporting\n\nDashboards track metrics such as cycle time, loss ratios, settlement patter...",
          "Type: products<br>Text: Pricing Tiers\n\nClaimllm offers Core, Advanced, and Enterprise tiers to suit insurers of varying size...",
          "Type: products<br>Text: Roadmap Overview\n\nThe roadmap outlines major releases from version 1.0 in Q1‚ÄØ2025 to advanced AI‚Äëdri...",
          "Type: products<br>Text: Product Overview\n\nLifellm is an advanced life insurance platform created by Insurellm that modernize...",
          "Type: products<br>Text: AI-Powered Underwriting\n\nThe AI engine evaluates health records, lifestyle data, and demographics to...",
          "Type: products<br>Text: Predictive Risk Modeling\n\nMachine learning models assess mortality risk using extensive datasets suc...",
          "Type: products<br>Text: Digital Health Integration\n\nLifellm connects with wearables and health apps to offer dynamic policie...",
          "Type: products<br>Text: Automated Policy Management\n\nThe platform provides tools for beneficiary management, premium calcula...",
          "Type: products<br>Text: Smart Document Processing\n\nNatural language processing and OCR automatically extract and validate da...",
          "Type: products<br>Text: Customer Portal\n\nA user-friendly portal lets policyholders manage coverage, update beneficiaries, an...",
          "Type: products<br>Text: Compliance Automation\n\nBuilt‚Äëin tools keep policies aligned with state and federal regulations, upda...",
          "Type: products<br>Text: Pricing\n\nLifellm offers tiered pricing to suit insurers of any size, from a starter tier at $3,500/m...",
          "Type: products<br>Text: Roadmap\n\nThe roadmap outlines major releases from version 1.0 in Q2‚ÄØ2025 to international expansion ...",
          "Type: products<br>Text: Healthllm Summary\n\nHealthllm is Insurellm's comprehensive health insurance platform that empowers pr...",
          "Type: products<br>Text: Intelligent Plan Design\n\nThe AI-powered tools help insurers design competitive health plans by analy...",
          "Type: products<br>Text: Real-Time Eligibility Verification\n\nIntegrated eligibility checking provides instant verification of...",
          "Type: products<br>Text: AI-Driven Claims Adjudication\n\nAutomated claims processing uses machine learning to review claims fo...",
          "Type: products<br>Text: Predictive Healthcare Analytics\n\nAdvanced analytics identify high-risk members who would benefit fro...",
          "Type: products<br>Text: Provider Network Management\n\nComprehensive tools manage provider relationships, contract negotiation...",
          "Type: products<br>Text: Member Engagement Platform\n\nA mobile-first portal gives members easy access to coverage information,...",
          "Type: products<br>Text: Medication Management\n\nIntegration with pharmacy benefit managers enables formulary management, prio...",
          "Type: products<br>Text: Regulatory Compliance Engine\n\nThe built-in compliance monitoring ensures adherence to ACA, state man...",
          "Type: products<br>Text: Pricing Overview\n\nHealthllm offers tiered pricing with Essential, Professional, and Enterprise optio...",
          "Type: products<br>Text: Roadmap Overview\n\nThe roadmap outlines key releases from version 1.0 in Q1 2025 to global expansion ...",
          "Type: products<br>Text: Product Overview\n\nMarkellm is a two-sided marketplace that connects consumers with insurance compani...",
          "Type: products<br>Text: Key Features\n\nThe platform provides AI-powered matching, a user-friendly interface, real‚Äëtime quotes...",
          "Type: products<br>Text: Pricing Model\n\nMarkellm offers free membership for consumers plus premium subscription options, whil...",
          "Type: products<br>Text: 2025 Q1 Roadmap\n\nIn Q1 2025, Markellm will launch a mobile app and introduce a referral program to e...",
          "Type: products<br>Text: 2025 Q2 Roadmap\n\nQ2 2025 focuses on expanding product offerings to life and health insurance and par...",
          "Type: products<br>Text: 2025 Q3 Roadmap\n\nDuring Q3 2025, a comprehensive marketing campaign and the release of testimonials ...",
          "Type: products<br>Text: 2026 Q4 Roadmap\n\nQ4 2026 will implement machine learning enhancements to improve match precision and...",
          "Type: products<br>Text: Conclusion\n\nMarkellm aims to improve the insurance experience through technology and insights, aspir...",
          "Type: employees<br>Text: Personal Summary\n\nPriya Sharma was born on January 8, 1986 and currently works as a Senior Data Scie...",
          "Type: employees<br>Text: Career Progression\n\nPriya has been a Senior Data Scientist at Insurellm since March 2018, leading ML...",
          "Type: employees<br>Text: Annual Performance History\n\nPriya consistently receives high performance ratings, reaching 4.9/5 in ...",
          "Type: employees<br>Text: Compensation History\n\nPriya's compensation has grown steadily, reaching a base salary of $145,000 wi...",
          "Type: employees<br>Text: Other HR Notes\n\nPriya holds a PhD from Stanford, has authored over seven peer‚Äëreviewed papers, and i...",
          "Type: employees<br>Text: Personal Summary\n\nKevin Zhang was born on March 27, 1990 and works as a Mobile Developer in San Fran...",
          "Type: employees<br>Text: Career Progression\n\nSince May 2020 Kevin has led iOS development for Insurellm's consumer mobile app...",
          "Type: employees<br>Text: Annual Performance History\n\nKevin received a 4.6/5 rating in 2023 for launching the Android app on t...",
          "Type: employees<br>Text: Compensation History\n\nKevin's base salary increased from $105,000 in 2020 to $128,000 in 2023, accom...",
          "Type: employees<br>Text: Additional HR Notes\n\nKevin holds a BS in Computer Science from UC Berkeley and is recognized for his...",
          "Type: employees<br>Text: Employee Summary\n\nSamuel Trenton, born April 12, 1989, holds the position of Senior Data Scientist i...",
          "Type: employees<br>Text: Career Progression\n\nSamuel advanced from Junior Data Analyst in 2016 to Senior Data Scientist by 202...",
          "Type: employees<br>Text: Performance History\n\nFrom 2020 to 2023 Samuel's performance ratings ranged from 3.0 to 4.5, with not...",
          "Type: employees<br>Text: Compensation & Additional HR Notes\n\nSamuel's compensation grew from $100,000 base salary in 2020 to ...",
          "Type: employees<br>Text: Employee Summary\n\nMarcus Johnson is a 35-year-old Customer Success Manager based in New York, earnin...",
          "Type: employees<br>Text: Career Progression\n\nSince April 2020, Marcus has served as a Customer Success Manager, overseeing 25...",
          "Type: employees<br>Text: Annual Performance History\n\nMarcus received a 4.7/5 performance rating in 2023, highlighted by excep...",
          "Type: employees<br>Text: Compensation History\n\nHis compensation has risen from a $65,000 base salary in 2018 to $98,000 in 20...",
          "Type: employees<br>Text: Additional HR Notes\n\nMarcus holds a BA in Business Administration from NYU Stern and certifications ...",
          "Type: employees<br>Text: Employee Profile Summary\n\nSamantha Greene is an HR Generalist based in Denver, born on October 14, 1...",
          "Type: employees<br>Text: Career Progression at Insurellm\n\nFrom 2020 to 2023 Samantha advanced from HR Coordinator to a pivota...",
          "Type: employees<br>Text: Performance History 2020-2022\n\nPerformance ratings varied, with an outstanding 2020, a solid 2021, a...",
          "Type: employees<br>Text: Performance & Compensation 2023\n\nIn 2023 Samantha met expectations after conflict‚Äëresolution trainin...",
          "Type: employees<br>Text: Additional HR Contributions\n\nSamantha is pursuing SHRM‚ÄëCP certification, participates in wellness pr...",
          "Type: employees<br>Text: HR Record Overview\n\nThe document provides Alex Thomson‚Äôs basic personal details and outlines his car...",
          "Type: employees<br>Text: Performance and Compensation\n\nAlex Thomson‚Äôs performance history shows consistently high ratings, wi...",
          "Type: employees<br>Text: Additional HR Notes\n\nAdditional HR notes emphasize Alex‚Äôs involvement in diversity initiatives, adva...",
          "Type: employees<br>Text: HR Overview\n\nJennifer Adams was born on July 7, 1997 and works as a remote Sales Development Represe...",
          "Type: employees<br>Text: Career Progression\n\nSince March 2023, Jennifer has been a Sales Development Representative generatin...",
          "Type: employees<br>Text: Performance & Compensation\n\nIn 2023 she received a performance rating of 3.0/5, meeting basic expect...",
          "Type: employees<br>Text: Additional HR Notes\n\nJennifer holds a BA in Business Administration from the University of Denver an...",
          "Type: employees<br>Text: Personal Details & Early Career\n\nJordan Blake was born on March 15, 1993 and works as a Sales Develo...",
          "Type: employees<br>Text: Mid Career Achievements & Performance\n\nJordan was recognized as SDR of the Month for three consecuti...",
          "Type: employees<br>Text: Recent Performance, Compensation & HR Notes\n\nIn 2023 Jordan set to exceed annual targets by 30% in Q...",
          "Type: company<br>Text: Founding and Early Growth\n\nInsurellm was founded in 2015 by Avery Lancaster as an insurance tech sta...",
          "Type: company<br>Text: Strategic Restructuring and Lean Operations\n\nBetween 2022 and 2023 the company restructured to prior...",
          "Type: company<br>Text: Product Expansion and Market Reach\n\nPost‚Äërestructuring, Insurellm broadened its suite to eight platf..."
         ],
         "type": "scatter3d",
         "x": {
          "bdata": "1tLMQYZXQkKz6nTB4E5WQUt5zj6+GBZBeyciQuwZB0APp/xAXpiiQevYSUIWgOzAgYDqQBdQhEGnMxdCnkyywWANDkEFV09ARJjLQbP900ECt0NC/s1YwSXa50BmcSNBLwepQPuuWUKWXzDBi3RZQKaF50EzqVhBNQ8iQiivrMH2TpTBoe1QQdzk6UCGTONBsb84QhlSXcGRSEtBXeNXQfm3oEFqMVdCU1FLwftEZ0GK/bdBwqHXQXPFssDJTAtBj2eBQcFnKUIRL4bBCNT0QCDMEUAkiuhBq69MQtL2KMEtUp5ANB8TQPcx8EFIyjtCHBRKwFpZfEHjAnBBEgI1Quxcj8EJftFAzNx/QS22i0HU83VBV50uQsDatsFzDFtB4qEfQUDo3EF6dhxCmx+5wZN6oUHejIJBbHRoQRKiI0LepsxArbUSQqpqr7/gzQFBROkdQFLPtEFPnktCgfkxwYpMn0ExOTNAp9F6QfaTjkEYxE3Bx+KKwF09KEFgYQtBd2fEQaVLnkE8YiBCG6SawRvoa0Hu6KlBmji0QVfFtUBKri1CgIBFwZ6pA0B/Bw1BHrgjQZl4HkGpGQRCOraFwE8c4UArdUZBj+JDQQy2QEIITTDAq/7KPxXE+0AcD4dBcCo4QgeyFME0dpBAMLULQap8o0Hrd05Coi2pwYlAWkF4BIhB6P9KQci8BUJp5Z2/HzESwaQzokBhFE9B1hOfQSxng0Gkt8ZBMqQowA2gFsGhWpM/FjSEwPL85kGcWxFC4saBweUQTkDFCVdBhws9QZC8DULd9pPBPeQQQQiKPkFdtzpB3LmxQQ/9S0IA4mrBQlGQQTsTz0EZaDFCGIgTwZTDIz9roIlAvZ+gQd7Nq0Guh3RBe9AiQqPhE8G3VARBZqvZQEZOSsFPjZfBjtdwwOSa48C17JTByOrfwB8OL8GDvDbBMk2Cwdopy8GAwkFAJyK1wIGzIcBBY4TAo3ukQF1BcMEUBKK/OOaywBnQyEBgFjvBrDxwwXUDfcGNqQpAJs0lwa4Lw8Hj/6nB8SnnwV3SSsHK7wLC6Uebwb1UBMKJ0GnBjDHuwcdmTMHEYwzBZBS+QCmuocGhk1TBrhq8warkxMHqUYbB6oDDwVXyn8EyQanBJoW5weyl/kDEf07BdAO8wc4P1MHlseLBxlnqwesHm8EYnTTB5du6wZZ22cEkR67BnCudQCbVscEvLpjBh3GGwcNQ30H3x5HB5DY2wadJksHbbxHB/aWYwfIIT8IiVwzCUSIAwvM9TMLR6fLB/H43wgSLHsLXvxPCYfwswtbwIcKoQVLCOSYiwkVZ98FY3jbCFPZOwviUPcIWm0vC5f40wv23PsKR9S7CzY4gwiQIEMImyiDCYbohwv5RIcL7Nw/Cl9odwmuPNsJtRx7C+MAXwgsAF8JwRjrCJPwiwtrHLcKd+4rAsunawKhUDcE=",
          "dtype": "f4"
         },
         "y": {
          "bdata": "fvO4wDf6V8BecDrBUU2YQI8KFEBHu5bAoHKFvjgLEsFvz9LA6AEuQXp3EEFd7T1Ao5QBQZQRfryf7ohAiq5NvxHR48AfZtDAHtSSv/t8Iz/Dv3hBOTwfwfT4HsHuoKnAtIUdQP03iUCNPYDB7DIPvzlvSsAgeSBBeASDQclrmsFaf5TB/5WQwaZ6GcFHQqhASeZyQQBYcMFeVxTBciIYwdrAPsGgBRBA6PiiwDVRzz8w1DFBONt5QfIiD7/KIF7Bc1WpQNh/MEFA6LA/P8wUwby7OcGci8VA/E4dQV456cAzyu7A2HvFvX0hMsCSF0BAZRFJwcMS8b9Qffc/CZAKQZw9jT/HQiVAqm04QOc1HMD5ToZBkFoVQcTf58Cowbq/HpfBwLzGj0FsaEVBKKhRwcwZ3sBY69PA+IC8v78Vk0Bly15At/8KwRUv+sDV7k+/N/EXQIC628BHHQdBdWCLvlVLI0ANLsFAZ6RUQd2ZkkE9aHvBU9WRwVlaY8GWgU/BI54pwIjjpEFQqohAZAsnwVvYD8E4U9xA+esdQS6lGUHcRrq/sykzv9WTqcCc71HA//ysQAtsZ0GI3aVB9NoZwSsamsHRn4zBP6eHwV/JW0BcVMRAnBjdQDNmhsH5V5fBSOKEQJwAPEGJiVFBXJ+BQIfFZ0BeJl5BLwHAP1HRbsHrkmfBnYdHQcv1VEFnoHtAm/wNv4IAS8E0sWvBrpCeQY7pgUH/2IRBdo+bwEnj08AZtmzBzglEwV6df8CzgajAl7mRwcIkgcF+AUxAeJurQfcoMkFftzjBNksywOFd/79cZbfAL4sZwP+w5r8wrj/B5p15wI8kJ0EdaohB1hfrwDGNQ8BfZX3AjDgEwYNoE8HXdg3BG0RtwEbYWb9SlXa/LBwHQImhA0FxeRJBfWmkwdyptkBBaCbAA6EwwUEo90Bqv6vB6bbQweuqAkHe3HRBDLEOQTvikb8+2tPAmc4aQSt7NEEdCF5BQpYfQRxYZ0EW8nxBG9PaQJEsEUD5eX5Bv7HoQPCeTkDwKOpAKBj7QEIWhb/AizBBcHVXQDXFur+S8LPBOp4jQFImDEGvHMjBGRumQV0dA0EJuVVBtAY4QReYVMHPca5B9GmMwbft/EBYk+3BTX6IwcCQf0EQmYpBka0IwGOklMCVxgjBbW6fQJJxgMF3t8fBlxvywYzFlsG++ILBQgqoQdWP68C3BqvAO0DVwIUFQEBF5KNBICWcQYWjr0Fi369BvckFvpVYGsFwYPhAPQyMQFz7PMGOVAxBzGRPwWr5jcGCwKbBjiFrwUv9e8HKOFTAhzTFQArVCkBiAbzAs7s1QMSxOUEHKQFB98M5wer2O0GUnKs/xHjAQN1q1j8yqAvACEkNQQF1J8Dys6rAmanmwNDddcABphpBAaoBwUr6UkEkIQ/AHEEmQEZ2hcAGXINBhqSwQXRBd0E=",
          "dtype": "f4"
         },
         "z": {
          "bdata": "Xk2hwPFhEUFH7J2+v/bnwb2czUEMo0PA9ghvQfbr+7+quADBP3ULwYqSXEFoy3XBp3OmwSINHb+beDpBUn7MweHwwcE1TvxBSU9UQYhw3UBExQ1BVyDDwZxgGMLijgVBVTpYQW1LxT9A9n9A8mAQwmJ1g8HQ6gdB1KE2QYXnvMFTfYvBGTLEwXihwkHAMfI/YUuLwPZ1x8GRVgbCkPwJQh3sycDmKhVBhsffwLfAisEYGYdAyezHQKPsfEFfuOPBG1SxQDl/g0FqbWzBEd+PwQeg4UFairfAyna9wCTJpUHwK+/B29XxQMruKr43LLhAOY8hPr1fvMFsCLLACdq7P7sdvsFkRuHBR6XvQQYKhUFomQpBVXsCQebDmcHS5QjC9kDiQQEZAMEKPZq+IxN7wbkFBcKZg9hBO0omQQWuaMCW4gfAlbW3QImJu0B1yKnB0m4PQnupYj939L9AC29nwIRR4MFkofJB97/CP+Dmqb/USI/BdwD1wW8elcEflOlBgOaSQa++8EAro5xAQBDgweoaxMEplWtBrAoeQWp0iEFjkpg/I7GFQaw/q8HKbxBCXMF8QMwIoUE4wRxBVI4vQS31scFFfVZBDYfxQA48t7+6tYfBEdqmwWxIh0GeKwxA4yFJQVZShcGQLsLB88oXQuEXar5Lx5I+kOiVwTgdCcK4J5NB6NAdwcDwiEEJcynBNqmrwc41scFBDMNBqgSgQXuWpEEWW6FBaysuwWL3ccExYX/Bk5mEwRWYEcGupg3BgmhRQC+E/MFR5YxBGgsnQTiZ4kB25Y7BG5HpwXaAwEEeQ31B+EI0wXJoAUDo2tlA1lB2wThTQMCzuxxAKq9/QZWd2cG3Qr9BkzFMQXnztkChmL4/P/TTQPuCFcGsL27B3gr5QdHiKUGKvztBgQn4Pw3kB0HsYJ5B37YJwT7zkUETXt3A1/V6QIIzq8AoWLRAustMP/VyWEDwEKw/8Kw5Px+JC8GVS0jBRIw+wTdSDcHRKU3BzlBYPxsWhMBLPLm/3u9awAj8K8EPVNXBCK2cwa6XBcIiRmfB9tMSwmHgWcEPblxBFOjNwd640sE3iA/BMfJjwEzvgMFdNYVAFz8CQbz5pcDA9+g/vX5PQf83FMJpxa2/iASSQWRWOz+Ei1S/GePowOXSMMCCV7jBt9JkwbkKDsF8ISNBhLmuwAWGbcGN1LVBrvcwQNwIMsEnEvxAZa9LQHHbgsHbWoPBhKj2wJPKLsGFuWfB6VSkQL4m97/veJ9BGfd7QduBhkB3W6lBm1yuwMr6FsHaBuDAqVIBQLt5ZcFGxgE+Zb2mQfrTJUFAWjdA4L2XQJQHSUHre0VBpQzCQEFx0EAI2izA4HZLQGhmTEDQM68/cV3Kv5jihUFzvCJBl+ZnQeJitsCBoj5Bnf5WQNzRB0Ft3mtB0qM9QTTeCUHVXVVBk3dgQUqsGkE=",
          "dtype": "f4"
         }
        }
       ],
       "layout": {
        "height": 700,
        "margin": {
         "b": 10,
         "l": 10,
         "r": 10,
         "t": 40
        },
        "scene": {
         "xaxis": {
          "title": {
           "text": "x"
          }
         },
         "yaxis": {
          "title": {
           "text": "y"
          }
         },
         "zaxis": {
          "title": {
           "text": "z"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "3D Chroma Vector Store Visualization"
        },
        "width": 900
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    z=reduced_vectors[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=10, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af77948",
   "metadata": {},
   "source": [
    "## And now - let's build an Advanced RAG!\n",
    "\n",
    "We will use these techniques:\n",
    "\n",
    "1. Reranking - reorder the rank results\n",
    "2. Query re-writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "7356bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankOrder(BaseModel):\n",
    "    order: list[int] = Field(\n",
    "        description=\"The order of relevance of chunks, from most relevant to least relevant, by chunk id number\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8302ff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rerank(question, chunks):\n",
    "#     system_prompt = \"\"\"\n",
    "# You are a document re-ranker.\n",
    "# You are provided with a question and a list of relevant chunks of text from a query of a knowledge base.\n",
    "# The chunks are provided in the order they were retrieved; this should be approximately ordered by relevance, but you may be able to improve on that.\n",
    "# You must rank order the provided chunks by relevance to the question, with the most relevant chunk first.\n",
    "# Reply only with the list of ranked chunk ids, nothing else. Include all the chunk ids you are provided with, reranked.\n",
    "# \"\"\"\n",
    "#     user_prompt = f\"The user has asked the following question:\\n\\n{question}\\n\\nOrder all the chunks of text by relevance to the question, from most relevant to least relevant. Include all the chunk ids you are provided with, reranked.\\n\\n\"\n",
    "#     user_prompt += \"Here are the chunks:\\n\\n\"\n",
    "#     for index, chunk in enumerate(chunks):\n",
    "#         user_prompt += f\"# CHUNK ID: {index + 1}:\\n\\n{chunk.page_content}\\n\\n\"\n",
    "#     user_prompt += \"Reply only with the list of ranked chunk ids, nothing else.\"\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": system_prompt},\n",
    "#         {\"role\": \"user\", \"content\": user_prompt},\n",
    "#     ]\n",
    "#     response = completion(model=MODEL, messages=messages, response_format=RankOrder)\n",
    "#     reply = response.choices[0].message.content\n",
    "#     order = RankOrder.model_validate_json(reply).order\n",
    "#     print(order)\n",
    "#     return [chunks[i - 1] for i in order]\n",
    "\n",
    "def rerank(question, chunks):\n",
    "    system_prompt = \"\"\"\n",
    "You are a document re-ranker.\n",
    "You are provided with a question and a list of relevant chunks of text from a query of a knowledge base.\n",
    "The chunks are provided in the order they were retrieved; this should be approximately ordered by relevance, but you may be able to improve on that.\n",
    "You must rank order the provided chunks by relevance to the question, with the most relevant chunk first.\n",
    "Reply only with the list of ranked chunk ids, nothing else. Include all the chunk ids you are provided with, reranked.\n",
    "\"\"\"\n",
    "\n",
    "    user_prompt = f\"The user has asked the following question:\\n\\n{question}\\n\\n\"\n",
    "    user_prompt += \"Order all the chunks of text by relevance.\\n\\nHere are the chunks:\\n\\n\"\n",
    "\n",
    "    for index, chunk in enumerate(chunks):\n",
    "        user_prompt += f\"# CHUNK ID: {index + 1}:\\n\\n{chunk.page_content}\\n\\n\"\n",
    "\n",
    "    user_prompt += \"Reply only with the list of ranked chunk ids, nothing else.\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    response = completion(model=MODEL, messages=messages, response_format=RankOrder)\n",
    "    reply = response.choices[0].message.content\n",
    "\n",
    "    order = RankOrder.model_validate_json(reply).order\n",
    "    print(\"Model order:\", order)\n",
    "\n",
    "    # ‚úÖ SAFETY FILTER (minimal fix)\n",
    "    valid_ids = [i for i in order if 1 <= i <= len(chunks)]\n",
    "\n",
    "    return [chunks[i - 1] for i in valid_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "6a8120bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETRIEVAL_K = 10\n",
    "\n",
    "# def fetch_context_unranked(question):\n",
    "#     query = openai.embeddings.create(model=embedding_model, input=[question]).data[0].embedding\n",
    "#     results = collection.query(query_embeddings=[query], n_results=RETRIEVAL_K)\n",
    "#     chunks = []\n",
    "#     for result in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "#         chunks.append(Result(page_content=result[0], metadata=result[1]))\n",
    "#     return chunks\n",
    "\n",
    "RETRIEVAL_K = 10\n",
    "\n",
    "def fetch_context_unranked(question):\n",
    "    # create query embedding locally\n",
    "    query = embedding_model.encode(question).tolist()\n",
    "\n",
    "    results = collection.query(query_embeddings=[query], n_results=RETRIEVAL_K)\n",
    "\n",
    "    chunks = []\n",
    "    for result in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "        chunks.append(Result(page_content=result[0], metadata=result[1]))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6d670d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who won the IIOTY award?\"\n",
    "chunks = fetch_context_unranked(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "6f24a763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional HR Notes\n",
      "\n",
      "Marcus holds a BA in Business Administration from NYU Stern and certifications including CCSM and Salesforce Administrator. He has earned the 2023 Customer Champion Award and actively mentors new associates, noted for strong relationship building and strategic advising.\n",
      "\n",
      "## Other HR Notes\n",
      "- **Education:** BA in Business Administration from NYU Stern School of Business\n",
      "- **Certifications:** Certified Customer Success Manager (CCSM), Salesforce Administrator\n",
      "- **Recognition:** Customer Champion Award 2023 for highest NPS scores\n",
      "- **Mentorship:** Actively mentors new Customer Success Associates\n",
      "- **Feedback:** Excellent relationship builder with deep product knowledge. Strong strategic advisor to clients. Known for going above and beyond to ensure client success.\n",
      "Career Progression at Insurellm\n",
      "\n",
      "From 2020 to 2023 Samantha advanced from HR Coordinator to a pivotal role in diversity and inclusion initiatives. Her responsibilities grew to include recruitment, employee relations, and mentorship program leadership.\n",
      "\n",
      "## Insurellm Career Progression\n",
      "- **2020** - Joined Insurellm as a HR Coordinator\n",
      "  - Responsibilities included assisting with recruitment processes and managing employee onboarding.\n",
      "- **2021** - Promoted to HR Generalist\n",
      "  - Transitioned to a role with expanded responsibilities, including handling employee relations and benefits administration.\n",
      "- **2022** - Completed the HR Leadership Development Program\n",
      "  - Enhanced skills in conflict resolution and strategic planning.\n",
      "- **2023** - Actively involved in initiating the company‚Äôs Diversity and Inclusion programs.\n",
      "  - Samantha Greene played a key role in launching mentorship initiatives and employee resource groups.\n",
      "\n",
      "## Annual Performance History\n",
      "Employee Summary\n",
      "\n",
      "Samuel Trenton, born April 12, 1989, holds the position of Senior Data Scientist in Austin, Texas with a current salary of $115,000. The record lists his key personal and job details.\n",
      "\n",
      "# HR Record\n",
      "\n",
      "# Samuel Trenton\n",
      "\n",
      "## Summary\n",
      "- **Date of Birth:** April 12, 1989\n",
      "- **Job Title:** Senior Data Scientist\n",
      "- **Location:** Austin, Texas\n",
      "- **Current Salary:** $115,000  \n",
      "Employee Summary\n",
      "\n",
      "Marcus Johnson is a 35-year-old Customer Success Manager based in New York, earning a salary of $98,000. His personal details include a birth date of February 17, 1988.\n",
      "\n",
      "# HR Record\n",
      "\n",
      "# Marcus Johnson\n",
      "\n",
      "## Summary\n",
      "- **Date of Birth:** February 17, 1988\n",
      "- **Job Title:** Customer Success Manager\n",
      "- **Location:** New York, New York\n",
      "- **Current Salary:** $98,000\n",
      "Annual Performance History\n",
      "\n",
      "Marcus received a 4.7/5 performance rating in 2023, highlighted by exceptional client satisfaction and account expansion. Ratings in previous years ranged from 4.0 to 4.3, reflecting steady performance with occasional challenges.\n",
      "\n",
      "## Annual Performance History\n",
      "- **2023:** Rating: 4.7/5\n",
      "  *Exceptional performance with highest client satisfaction scores in the team. Successfully expanded three key accounts.*\n",
      "\n",
      "- **2022:** Rating: 4.3/5\n",
      "  *Strong performance with good retention numbers. Faced challenges with one difficult client but resolved successfully.*\n",
      "\n",
      "- **2021:** Rating: 3.8/5\n",
      "  *Solid year but missed expansion revenue targets in Q2 and Q3. Improved significantly in Q4.*\n",
      "\n",
      "- **2020:** Rating: 4.0/5\n",
      "  *Good transition to CSM role. Quickly built rapport with enterprise clients and demonstrated strategic thinking.*\n",
      "HR Record Overview\n",
      "\n",
      "The document provides Alex Thomson‚Äôs basic personal details and outlines his career progression at Insurellm, highlighting his rapid adaptation and leadership roles. It covers his start as an SDR in November 2022 and subsequent promotions and training contributions.\n",
      "\n",
      "# HR Record\n",
      "\n",
      "# Alex Thomson\n",
      "\n",
      "## Summary\n",
      "- **Date of Birth:** March 15, 1995\n",
      "- **Job Title:** Sales Development Representative (SDR)\n",
      "- **Location:** Austin, Texas\n",
      "- **Current Salary:** $65,000  \n",
      "\n",
      "## Insurellm Career Progression\n",
      "- **November 2022** - Joined Insurellm as a Sales Development Representative. Alex Thomson quickly adapted to the team, demonstrating exceptional communication and rapport-building skills.\n",
      "- **January 2023** - Promoted to Team Lead for special projects due to Alex's initiative in driving B2B customer outreach programs.  \n",
      "- **August 2023** - Developed a training module for new SDRs at Insurellm, enhancing onboarding processes based on feedback and strategies that Alex Thomson pioneered.  \n",
      "- **Current** - Continues to excel in the role, leading a small team of 5 SDRs while collaborating closely with the marketing department to identify new lead-generation strategies.  \n",
      "Mid Career Achievements & Performance\n",
      "\n",
      "Jordan was recognized as SDR of the Month for three consecutive months in late 2022 and participated in a leadership training program in 2023. His performance in 2021 and 2022 showed strong growth, achieving 90% of targets the first year and 120% the next, with innovative outreach strategies praised by leadership.\n",
      "\n",
      "## Insurellm Career Progression\n",
      "- **2022-12:** Recognized as SDR of the Month for three consecutive months  \n",
      "- **2023-05:** Participated in the Insurellm Leadership Training Program  \n",
      "\n",
      "## Annual Performance History\n",
      "- **2021:** First year at Insurellm; achieved 90% of monthly targets.  \n",
      "  - **Feedback:** Strong potential shown in lead generation; needs improvement in follow-up techniques.  \n",
      "- **2022:** Achieved 120% of targets; pioneered outreach strategies that increased customer engagement.  \n",
      "  - **Feedback:** Jordan's innovative approach contributed significantly to team success; recommended for leadership training.  \n",
      "Additional HR Notes\n",
      "\n",
      "Additional HR notes emphasize Alex‚Äôs involvement in diversity initiatives, advanced CRM training, and continuous professional development. Peers recognize his supportive team environment and his future potential within the company.\n",
      "\n",
      "## Compensation History\n",
      "- **2023**: Base Salary - $65,000 | Bonus - $10,000 (for exceeding sales targets and exceptional teamwork)  \n",
      "- **Projected for 2024**: Anticipated salary increase due to Alex Thomson's significant contributions and successful completion of leadership training.\n",
      "\n",
      "## Other HR Notes\n",
      "- Alex Thomson is an active member of the Diversity and Inclusion committee at Insurellm and has participated in various community outreach programs.  \n",
      "- Alex has received external training on advanced CRM usage, which has subsequently improved team efficiency and productivity.\n",
      "- Continuous professional development through attending sales conventions and workshops, with plans to pursue certification in Sales Enablement in 2024.\n",
      "- Recognized by peers for promoting a supportive and high-energy team environment, often organizing team-building activities to enhance camaraderie within the SDR department. \n",
      "\n",
      "--- \n",
      "**Comment:** Alex Thomson is considered a cornerstone of Insurellm‚Äôs sales team and has a bright future within the organization.\n",
      "Performance History 2020-2022\n",
      "\n",
      "Performance ratings varied, with an outstanding 2020, a solid 2021, and a below‚Äëexpectations rating in 2022 due to employee‚Äërelations challenges. These evaluations guided subsequent training and development.\n",
      "\n",
      "## Annual Performance History\n",
      "- **2020:** Exceeds Expectations  \n",
      "  Samantha Greene demonstrated exceptional organizational skills and contributed to a streamlined onboarding process, earning commendations from senior leadership.\n",
      "\n",
      "- **2021:** Meets Expectations  \n",
      "  While proficient in her new role, Samantha Greene struggled with time management during peak recruitment seasons, resulting in occasional missed deadlines. \n",
      "\n",
      "- **2022:** Below Expectations  \n",
      "  Samantha Greene faced challenges in balancing employee relations issues, thereby impacting her performance. Gaps in communication and follow-up led to a push for additional training.\n",
      "\n",
      "## Compensation History\n",
      "Other HR Notes\n",
      "\n",
      "Priya holds a PhD from Stanford, has authored over seven peer‚Äëreviewed papers, and is a recognized speaker at ML conferences. She also co‚Äëinvented two pending patents and received the 2023 Data Science Excellence Award.\n",
      "\n",
      "## Other HR Notes\n",
      "- **Education:** PhD in Computer Science (Machine Learning) from Stanford University\n",
      "- **Publications:** 7+ peer-reviewed papers in ML conferences and journals\n",
      "- **Speaking:** Regular speaker at ML and InsurTech conferences. Represented Insurellm at 4 major conferences in 2023.\n",
      "- **Patents:** Co-inventor on 2 pending patents for insurance ML applications\n",
      "- **Recognition:** Data Science Excellence Award 2023, featured in InsureTech Innovation Magazine\n",
      "- **Skills:** Expert in Python, TensorFlow, PyTorch, scikit-learn, SQL, and cloud ML platforms\n",
      "- **Feedback:** World-class technical talent with strong business acumen. Natural leader who elevates entire team. Key retention priority.\n",
      "Employee Profile Summary\n",
      "\n",
      "Samantha Greene is an HR Generalist based in Denver, born on October 14, 1990, earning a current salary of $70,000. The profile outlines her basic personal and job information.\n",
      "\n",
      "# Samantha Greene\n",
      "\n",
      "## Summary\n",
      "- **Date of Birth:** October 14, 1990\n",
      "- **Job Title:** HR Generalist\n",
      "- **Location:** Denver, Colorado\n",
      "- **Current Salary:** $70,000\n",
      "\n",
      "## Insurellm Career Progression\n",
      "Compensation & Additional HR Notes\n",
      "\n",
      "Samuel's compensation grew from $100,000 base salary in 2020 to $115,000 in 2023, with bonuses reflecting performance. He engages in professional development, company culture activities, and seeks to improve engineering collaboration, while enjoying hiking and photography.\n",
      "\n",
      "## Compensation History\n",
      "- **2023:** Base Salary: $115,000 + Bonus: $15,000  \n",
      "  *Annual bonus based on successful project completions and performance metrics.*\n",
      "\n",
      "- **2022:** Base Salary: $110,000 + Bonus: $10,000  \n",
      "  *Slight decrease in bonus due to performance challenges during the year.*\n",
      "\n",
      "- **2021:** Base Salary: $105,000 + Bonus: $12,000  \n",
      "  *Merit-based increase, reflecting consistent contributions to the data science team.*\n",
      "\n",
      "- **2020:** Base Salary: $100,000 + Bonus: $8,000  \n",
      "  *Initial compensation as Senior Data Scientist, with a focus on building rapport with cross-functional teams.*\n",
      "\n",
      "## Other HR Notes\n",
      "- **Professional Development:** Completed several workshops on machine learning and AI applications in insurance. Currently pursuing an online certification in deep learning.\n",
      "\n",
      "- **Engagement in Company Culture:** Regularly participates in team-building events and contributes to the internal newsletter, sharing insights on data science trends.\n",
      "\n",
      "- **Areas for Improvement:** Collaboration with engineering teams has been noted as an area needing focus. Samuel has expressed a desire to work closely with tech teams to align data initiatives better.\n",
      "\n",
      "- **Personal Interests:** Has a keen interest in hiking and photography, often sharing his photography from weekend hikes with colleagues, fostering positive team relationships.\n",
      "Compensation History\n",
      "\n",
      "His compensation has risen from a $65,000 base salary in 2018 to $98,000 in 2023, complemented by increasing bonuses each year. The total remuneration reflects his growing responsibilities and performance.\n",
      "\n",
      "## Compensation History\n",
      "- **2023:** Base Salary: $98,000 + Bonus: $15,000\n",
      "- **2022:** Base Salary: $92,000 + Bonus: $12,000\n",
      "- **2021:** Base Salary: $85,000 + Bonus: $8,000\n",
      "- **2020:** Base Salary: $78,000 + Bonus: $10,000\n",
      "- **2019:** Base Salary: $68,000 + Bonus: $5,000\n",
      "- **2018:** Base Salary: $65,000\n",
      "Performance History\n",
      "\n",
      "From 2020 to 2023 Samuel's performance ratings ranged from 3.0 to 4.5, with notable achievements in AI-driven underwriting. While 2022 showed challenges, subsequent feedback and workshops helped improve collaboration.\n",
      "\n",
      "## Annual Performance History\n",
      "- **2023:** Rating: 4.5/5  \n",
      "  *Samuel exceeded expectations, successfully leading a cross-departmental project on AI-driven underwriting processes.*\n",
      "\n",
      "- **2022:** Rating: 3.0/5  \n",
      "  *Some challenges in meeting deadlines and collaboration with the engineering team. Received constructive feedback and participated in a team communication workshop.*\n",
      "\n",
      "- **2021:** Rating: 4.0/5  \n",
      "  *There was notable improvement in performance. Worked to enhance model accuracy, leading to improved risk assessment outcomes for B2C customers.*\n",
      "\n",
      "- **2020:** Rating: 3.5/5  \n",
      "  *Exhibited a solid performance during the initial year as a Senior Data Scientist but had struggles adapting to new leadership expectations.*\n",
      "Additional HR Contributions\n",
      "\n",
      "Samantha is pursuing SHRM‚ÄëCP certification, participates in wellness programs, and volunteers in community outreach, enhancing corporate social responsibility. Her ongoing development makes her a valuable asset to Insurellm.\n",
      "\n",
      "## Other HR Notes\n",
      "- Samantha Greene has expressed interest in pursuing an HR certification (SHRM-CP) to further her career growth within Insurellm. \n",
      "- Participated in Insurellm's employee wellness program, promoting mental health resources among staff.\n",
      "- Actively volunteers with local nonprofits and encourages staff involvement in community outreach programs, enhancing Insurellm's corporate social responsibility initiatives. \n",
      "\n",
      "Samantha Greene is a valuable asset to Insurellm, continuously working on professional development and contributing to a supportive workplace culture.\n",
      "Additional HR Notes\n",
      "\n",
      "Jennifer holds a BA in Business Administration from the University of Denver and is developing skills in Salesforce and LinkedIn Sales Navigator. Her development plan focuses on objection handling, while she aims to hit 100% quota in Q4‚ÄØ2024 and earn SDR of the Quarter recognition.\n",
      "\n",
      "## Other HR Notes\n",
      "- **Education:** BA in Business Administration from University of Denver (graduated 2022)\n",
      "- **Skills:** Learning Salesforce, LinkedIn Sales Navigator, and sales prospecting techniques\n",
      "- **Development Plan:** Enrolled in sales skills training program. Manager providing weekly coaching on objection handling and qualification techniques.\n",
      "- **Challenges:** Struggles with phone confidence and needs to improve research quality before outreach. Working on building pipeline consistency.\n",
      "- **Feedback:** Enthusiastic and coachable. Shows potential but needs more experience and skill development. Sometimes gives up too easily when facing rejection. Improving gradually with support.\n",
      "- **Goals:** Aiming to hit 100% of quota in Q4 2024 and qualify for SDR of the Quarter recognition\n",
      "Signatures\n",
      "\n",
      "The contract is signed by Michael Torres, CRO of Insurellm, Inc., and Jonathan Park, President & CEO of Guardian Life Partners, both dated March‚ÄØ1‚ÄØ2025. Their signatures formalize the partnership.\n",
      "\n",
      "**Signatures:**\n",
      "\n",
      "_________________________________\n",
      "**Michael Torres**\n",
      "**Title**: Chief Revenue Officer\n",
      "**Insurellm, Inc.**\n",
      "**Date**: March 1, 2025\n",
      "\n",
      "_________________________________\n",
      "**Jonathan Park**\n",
      "**Title**: President & CEO\n",
      "**Guardian Life Partners**\n",
      "**Date**: March 1, 2025\n",
      "Additional HR Notes\n",
      "\n",
      "Kevin holds a BS in Computer Science from UC Berkeley and is recognized for his expertise in Swift, Kotlin, and React Native. He earned the Mobile Innovation Award 2023, contributes to open‚Äësource projects, and speaks at industry conferences.\n",
      "\n",
      "## Other HR Notes\n",
      "- **Education:** BS in Computer Science from UC Berkeley\n",
      "- **Skills:** Expert in Swift, Kotlin, React Native, mobile UI/UX patterns, App Store optimization\n",
      "- **Recognition:** Mobile Innovation Award 2023 for Android app launch\n",
      "- **Open Source:** Active contributor to mobile development open-source projects\n",
      "- **Speaking:** Presented at Mobile DevCon 2023 on cross-platform development strategies\n",
      "- **Feedback:** Highly skilled mobile developer with excellent product sense. Takes ownership of mobile platform and drives continuous improvement. Strong mentor to other developers.\n",
      "Career Progression\n",
      "\n",
      "Since March 2023, Jennifer has been a Sales Development Representative generating qualified leads and booking discovery meetings. Prior roles include an SDR internship (June 2022‚ÄëFeb 2023) and a Customer Service position at RetailCo (2019‚Äë2022).\n",
      "\n",
      "## Insurellm Career Progression\n",
      "- **March 2023 - Present:** Sales Development Representative\n",
      "  - Generates qualified leads for enterprise sales team\n",
      "  - Conducts outbound prospecting via phone, email, and LinkedIn\n",
      "  - Researches target accounts in insurance industry\n",
      "  - Books discovery meetings for account executives\n",
      "\n",
      "- **June 2022 - February 2023:** SDR Intern\n",
      "  - Completed 9-month internship program\n",
      "  - Learned sales processes and CRM tools\n",
      "  - Shadowed senior SDRs and participated in training\n",
      "\n",
      "- **August 2019 - May 2022:** Customer Service Representative at RetailCo\n",
      "  - Handled customer inquiries and resolved issues\n",
      "  - Developed communication and problem-solving skills\n",
      "Career Progression\n",
      "\n",
      "Priya has been a Senior Data Scientist at Insurellm since March 2018, leading ML initiatives and mentoring junior staff. Previously she held data science roles at FinML Analytics and conducted research at UC Berkeley AI Lab.\n",
      "\n",
      "## Insurellm Career Progression\n",
      "- **March 2018 - Present:** Senior Data Scientist\n",
      "  - Leads machine learning initiatives for risk prediction models\n",
      "  - Built recommendation engine for Marketllm increasing conversion by 28%\n",
      "  - Mentors team of 3 junior data scientists\n",
      "  - Published 2 research papers on insurance ML applications\n",
      "\n",
      "- **June 2015 - February 2018:** Data Scientist at FinML Analytics\n",
      "  - Developed predictive models for financial services clients\n",
      "  - Specialized in customer churn prediction and fraud detection\n",
      "\n",
      "- **August 2012 - May 2015:** Research Scientist at UC Berkeley AI Lab\n",
      "  - Conducted research in machine learning and natural language processing\n",
      "  - Published 5 peer-reviewed papers\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk.page_content[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "6cf97816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model order: [23715468910]\n"
     ]
    }
   ],
   "source": [
    "reranked = rerank(question, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b58a06b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in reranked:\n",
    "    print(chunk.page_content[:15]+\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "eb536f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who went to Manchester University?\"\n",
    "RETRIEVAL_K = 20\n",
    "chunks = fetch_context_unranked(question)\n",
    "for index, c in enumerate(chunks):\n",
    "    if \"manchester\" in c.page_content.lower():\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e65dd259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model order: [1611018202345678911121314151719]\n"
     ]
    }
   ],
   "source": [
    "reranked = rerank(question, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f3421f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, c in enumerate(reranked):\n",
    "    if \"manchester\" in c.page_content.lower():\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "6e22df20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranker failed, using top retrieved chunk instead:\n",
      "Additional HR Notes\n",
      "\n",
      "Marcus holds a BA in Business Administration from NYU Stern and certifications including CCSM and Salesforce Administrator. He has earned the 2023 Customer Champion Award and actively mentors new associates, noted for strong relationship building and strategic advising.\n",
      "\n",
      "## Other HR Notes\n",
      "- **Education:** BA in Business Administration from NYU Stern School of Business\n",
      "- **Certifications:** Certified Customer Success Manager (CCSM), Salesforce Administrator\n",
      "- **Recognition:** Customer Champion Award 2023 for highest NPS scores\n",
      "- **Mentorship:** Actively mentors new Customer Success Associates\n",
      "- **Feedback:** Excellent relationship builder with deep product knowledge. Strong strategic advisor to clients. Known for going above and beyond to ensure client success.\n"
     ]
    }
   ],
   "source": [
    "if reranked:\n",
    "    print(reranked[0].page_content)\n",
    "else:\n",
    "    print(\"Reranker failed, using top retrieved chunk instead:\")\n",
    "    print(chunks[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "e6da667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_context(question):\n",
    "    chunks = fetch_context_unranked(question)\n",
    "    return rerank(question, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4715e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a knowledgeable, friendly assistant representing the company Insurellm.\n",
    "You are chatting with a user about Insurellm.\n",
    "Your answer will be evaluated for accuracy, relevance and completeness, so make sure it only answers the question and fully answers it.\n",
    "If you don't know the answer, say so.\n",
    "For context, here are specific extracts from the Knowledge Base that might be directly relevant to the user's question:\n",
    "{context}\n",
    "\n",
    "With this context, please answer the user's question. Be accurate, relevant and complete.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "a819e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the context, include the source of the chunk\n",
    "\n",
    "def make_rag_messages(question, history, chunks):\n",
    "    context = \"\\n\\n\".join(f\"Extract from {chunk.metadata['source']}:\\n{chunk.page_content}\" for chunk in chunks)\n",
    "    system_prompt = SYSTEM_PROMPT.format(context=context)\n",
    "    return [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "7a03a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rewrite_query(question, history=[]):\n",
    "#     \"\"\"Rewrite the user's question to be a more specific question that is more likely to surface relevant content in the Knowledge Base.\"\"\"\n",
    "#     message = f\"\"\"\n",
    "# You are in a conversation with a user, answering questions about the company Insurellm.\n",
    "# You are about to look up information in a Knowledge Base to answer the user's question.\n",
    "\n",
    "# This is the history of your conversation so far with the user:\n",
    "# {history}\n",
    "\n",
    "# And this is the user's current question:\n",
    "# {question}\n",
    "\n",
    "# Respond only with a single, refined question that you will use to search the Knowledge Base.\n",
    "# It should be a VERY short specific question most likely to surface content. Focus on the question details.\n",
    "# Don't mention the company name unless it's a general question about the company.\n",
    "# IMPORTANT: Respond ONLY with the knowledgebase query, nothing else.\n",
    "# \"\"\"\n",
    "#     response = completion(model=MODEL, messages=[{\"role\": \"system\", \"content\": message}])\n",
    "#     return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def rewrite_query(question, history=[]):\n",
    "    \"\"\"Rewrite the user's question to be a more specific question that is more likely to surface relevant content in the Knowledge Base.\"\"\"\n",
    "    message = f\"\"\"\n",
    "You are in a conversation with a user, answering questions about the company Insurellm.\n",
    "You are about to look up information in a Knowledge Base to answer the user's question.\n",
    "\n",
    "This is the history of your conversation so far with the user:\n",
    "{history}\n",
    "\n",
    "And this is the user's current question:\n",
    "{question}\n",
    "\n",
    "Respond only with a single, refined question that you will use to search the Knowledge Base.\n",
    "It should be a VERY short specific question most likely to surface content. Focus on the question details.\n",
    "Don't mention the company name unless it's a general question about the company.\n",
    "IMPORTANT: Respond ONLY with the knowledgebase query, nothing else.\n",
    "\"\"\"\n",
    "    response = completion(model=MODEL, messages=[{\"role\": \"system\", \"content\": message}])\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "d18dc236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who won the IIOTY award?'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewrite_query(\"Who won the IIOTY award?\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "2c9d86e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str, history: list[dict] = []) -> tuple[str, list]:\n",
    "    \"\"\"\n",
    "    Answer a question using RAG and return the answer and the retrieved context\n",
    "    \"\"\"\n",
    "    query = rewrite_query(question, history)\n",
    "    print(query)\n",
    "    chunks = fetch_context(query)\n",
    "    messages = make_rag_messages(question, history, chunks)\n",
    "    response = completion(model=MODEL, messages=messages)\n",
    "    return response.choices[0].message.content, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "8c41cddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who won the IIOTY award?\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "litellm.RateLimitError: RateLimitError: GroqException - {\"error\":{\"message\":\"Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kegx8yzce1fs824p93pwn7we` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199649, Requested 4072. Please try again in 26m47.472s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:218\u001b[0m, in \u001b[0;36mBaseLLMHTTPHandler._make_common_sync_call\u001b[0;34m(self, sync_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 218\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43msync_httpx_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m            \u001b[49m\u001b[43msigned_json_body\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msigned_json_body\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    224\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py:979\u001b[0m, in \u001b[0;36mHTTPHandler.post\u001b[0;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[0;32m--> 979\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py:961\u001b[0m, in \u001b[0;36mHTTPHandler.post\u001b[0;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[1;32m    960\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39msend(req, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[0;32m--> 961\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/httpx/_models.py:829\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 829\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/main.py:2126\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m             optional_params[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[0;32m-> 2126\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mbase_llm_http_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshared_session\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshared_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_get_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# model call logging done inside the class as we make need to modify I/O to fit aleph alpha's requirements\u001b[39;49;00m\n\u001b[1;32m   2142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgigachat\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2145\u001b[0m     \u001b[38;5;66;03m# GigaChat - Sber AI's LLM (Russia)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:521\u001b[0m, in \u001b[0;36mBaseLLMHTTPHandler.completion\u001b[0;34m(self, model, messages, api_base, custom_llm_provider, model_response, encoding, logging_obj, optional_params, timeout, litellm_params, acompletion, stream, fake_stream, api_key, headers, client, provider_config, shared_session)\u001b[0m\n\u001b[1;32m    519\u001b[0m     sync_httpx_client \u001b[38;5;241m=\u001b[39m client\n\u001b[0;32m--> 521\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_common_sync_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync_httpx_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_httpx_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43msigned_json_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigned_json_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m provider_config\u001b[38;5;241m.\u001b[39mtransform_response(\n\u001b[1;32m    533\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    534\u001b[0m     raw_response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m     json_mode\u001b[38;5;241m=\u001b[39mjson_mode,\n\u001b[1;32m    544\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:243\u001b[0m, in \u001b[0;36mBaseLLMHTTPHandler._make_common_sync_call\u001b[0;34m(self, sync_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 243\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:3825\u001b[0m, in \u001b[0;36mBaseLLMHTTPHandler._handle_error\u001b[0;34m(self, e, provider_config)\u001b[0m\n\u001b[1;32m   3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BaseLLMException(\n\u001b[1;32m   3820\u001b[0m         status_code\u001b[38;5;241m=\u001b[39mstatus_code,\n\u001b[1;32m   3821\u001b[0m         message\u001b[38;5;241m=\u001b[39merror_text,\n\u001b[1;32m   3822\u001b[0m         headers\u001b[38;5;241m=\u001b[39merror_headers,\n\u001b[1;32m   3823\u001b[0m     )\n\u001b[0;32m-> 3825\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m provider_config\u001b[38;5;241m.\u001b[39mget_error_class(\n\u001b[1;32m   3826\u001b[0m     error_message\u001b[38;5;241m=\u001b[39merror_text,\n\u001b[1;32m   3827\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mstatus_code,\n\u001b[1;32m   3828\u001b[0m     headers\u001b[38;5;241m=\u001b[39merror_headers,\n\u001b[1;32m   3829\u001b[0m )\n",
      "\u001b[0;31mOpenAIError\u001b[0m: {\"error\":{\"message\":\"Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kegx8yzce1fs824p93pwn7we` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199649, Requested 4072. Please try again in 26m47.472s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[232], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43manswer_question\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWho won the IIOTY award?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[231], line 7\u001b[0m, in \u001b[0;36manswer_question\u001b[0;34m(question, history)\u001b[0m\n\u001b[1;32m      5\u001b[0m query \u001b[38;5;241m=\u001b[39m rewrite_query(question, history)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(query)\n\u001b[0;32m----> 7\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m messages \u001b[38;5;241m=\u001b[39m make_rag_messages(question, history, chunks)\n\u001b[1;32m      9\u001b[0m response \u001b[38;5;241m=\u001b[39m completion(model\u001b[38;5;241m=\u001b[39mMODEL, messages\u001b[38;5;241m=\u001b[39mmessages)\n",
      "Cell \u001b[0;32mIn[214], line 3\u001b[0m, in \u001b[0;36mfetch_context\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfetch_context\u001b[39m(question):\n\u001b[1;32m      2\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m fetch_context_unranked(question)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrerank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[201], line 46\u001b[0m, in \u001b[0;36mrerank\u001b[0;34m(question, chunks)\u001b[0m\n\u001b[1;32m     39\u001b[0m user_prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReply only with the list of ranked chunk ids, nothing else.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     42\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_prompt},\n\u001b[1;32m     43\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_prompt},\n\u001b[1;32m     44\u001b[0m ]\n\u001b[0;32m---> 46\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRankOrder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m reply \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     49\u001b[0m order \u001b[38;5;241m=\u001b[39m RankOrder\u001b[38;5;241m.\u001b[39mmodel_validate_json(reply)\u001b[38;5;241m.\u001b[39morder\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/utils.py:1653\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[1;32m   1650\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[1;32m   1651\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[1;32m   1652\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[0;32m-> 1653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/utils.py:1517\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1517\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1518\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[1;32m   1520\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   1521\u001b[0m     call_type\u001b[38;5;241m=\u001b[39mcall_type,\n\u001b[1;32m   1522\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/main.py:4159\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[0m\n\u001b[1;32m   4156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m   4157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   4158\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 4159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4162\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4165\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2346\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[1;32m   2345\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, litellm_response_headers)\n\u001b[0;32m-> 2346\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   2347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2348\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mLITELLM_EXCEPTION_TYPES:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:365\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ExceptionCheckers\u001b[38;5;241m.\u001b[39mis_error_str_rate_limit(error_str):\n\u001b[1;32m    364\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\n\u001b[1;32m    366\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRateLimitError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    367\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    368\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m    369\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    370\u001b[0m     )\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ExceptionCheckers\u001b[38;5;241m.\u001b[39mis_error_str_context_window_exceeded(error_str):\n\u001b[1;32m    372\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: litellm.RateLimitError: RateLimitError: GroqException - {\"error\":{\"message\":\"Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kegx8yzce1fs824p93pwn7we` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199649, Requested 4072. Please try again in 26m47.472s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n"
     ]
    }
   ],
   "source": [
    "answer_question(\"Who won the IIOTY award?\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "2ae8bb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InsureLLM employees who attended Manchester University\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "litellm.RateLimitError: RateLimitError: GroqException - {\"error\":{\"message\":\"Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kegx8yzce1fs824p93pwn7we` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199356, Requested 4586. Please try again in 28m22.944s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:218\u001b[0m, in \u001b[0;36mBaseLLMHTTPHandler._make_common_sync_call\u001b[0;34m(self, sync_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 218\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43msync_httpx_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m            \u001b[49m\u001b[43msigned_json_body\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msigned_json_body\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    224\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py:979\u001b[0m, in \u001b[0;36mHTTPHandler.post\u001b[0;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[0;32m--> 979\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py:961\u001b[0m, in \u001b[0;36mHTTPHandler.post\u001b[0;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[1;32m    960\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39msend(req, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[0;32m--> 961\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/httpx/_models.py:829\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 829\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/main.py:2126\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m             optional_params[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[0;32m-> 2126\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mbase_llm_http_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshared_session\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshared_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_get_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# model call logging done inside the class as we make need to modify I/O to fit aleph alpha's requirements\u001b[39;49;00m\n\u001b[1;32m   2142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgigachat\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2145\u001b[0m     \u001b[38;5;66;03m# GigaChat - Sber AI's LLM (Russia)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:521\u001b[0m, in \u001b[0;36mBaseLLMHTTPHandler.completion\u001b[0;34m(self, model, messages, api_base, custom_llm_provider, model_response, encoding, logging_obj, optional_params, timeout, litellm_params, acompletion, stream, fake_stream, api_key, headers, client, provider_config, shared_session)\u001b[0m\n\u001b[1;32m    519\u001b[0m     sync_httpx_client \u001b[38;5;241m=\u001b[39m client\n\u001b[0;32m--> 521\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_common_sync_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync_httpx_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_httpx_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43msigned_json_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigned_json_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m provider_config\u001b[38;5;241m.\u001b[39mtransform_response(\n\u001b[1;32m    533\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    534\u001b[0m     raw_response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m     json_mode\u001b[38;5;241m=\u001b[39mjson_mode,\n\u001b[1;32m    544\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:243\u001b[0m, in \u001b[0;36mBaseLLMHTTPHandler._make_common_sync_call\u001b[0;34m(self, sync_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 243\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:3825\u001b[0m, in \u001b[0;36mBaseLLMHTTPHandler._handle_error\u001b[0;34m(self, e, provider_config)\u001b[0m\n\u001b[1;32m   3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BaseLLMException(\n\u001b[1;32m   3820\u001b[0m         status_code\u001b[38;5;241m=\u001b[39mstatus_code,\n\u001b[1;32m   3821\u001b[0m         message\u001b[38;5;241m=\u001b[39merror_text,\n\u001b[1;32m   3822\u001b[0m         headers\u001b[38;5;241m=\u001b[39merror_headers,\n\u001b[1;32m   3823\u001b[0m     )\n\u001b[0;32m-> 3825\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m provider_config\u001b[38;5;241m.\u001b[39mget_error_class(\n\u001b[1;32m   3826\u001b[0m     error_message\u001b[38;5;241m=\u001b[39merror_text,\n\u001b[1;32m   3827\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mstatus_code,\n\u001b[1;32m   3828\u001b[0m     headers\u001b[38;5;241m=\u001b[39merror_headers,\n\u001b[1;32m   3829\u001b[0m )\n",
      "\u001b[0;31mOpenAIError\u001b[0m: {\"error\":{\"message\":\"Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kegx8yzce1fs824p93pwn7we` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199356, Requested 4586. Please try again in 28m22.944s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[230], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43manswer_question\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWho went to Manchester University?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[228], line 7\u001b[0m, in \u001b[0;36manswer_question\u001b[0;34m(question, history)\u001b[0m\n\u001b[1;32m      5\u001b[0m query \u001b[38;5;241m=\u001b[39m rewrite_query(question, history)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(query)\n\u001b[0;32m----> 7\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m messages \u001b[38;5;241m=\u001b[39m make_rag_messages(question, history, chunks)\n\u001b[1;32m      9\u001b[0m response \u001b[38;5;241m=\u001b[39m completion(model\u001b[38;5;241m=\u001b[39mMODEL, messages\u001b[38;5;241m=\u001b[39mmessages)\n",
      "Cell \u001b[0;32mIn[214], line 3\u001b[0m, in \u001b[0;36mfetch_context\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfetch_context\u001b[39m(question):\n\u001b[1;32m      2\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m fetch_context_unranked(question)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrerank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[201], line 46\u001b[0m, in \u001b[0;36mrerank\u001b[0;34m(question, chunks)\u001b[0m\n\u001b[1;32m     39\u001b[0m user_prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReply only with the list of ranked chunk ids, nothing else.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     42\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_prompt},\n\u001b[1;32m     43\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_prompt},\n\u001b[1;32m     44\u001b[0m ]\n\u001b[0;32m---> 46\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRankOrder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m reply \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     49\u001b[0m order \u001b[38;5;241m=\u001b[39m RankOrder\u001b[38;5;241m.\u001b[39mmodel_validate_json(reply)\u001b[38;5;241m.\u001b[39morder\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/utils.py:1653\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[1;32m   1650\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[1;32m   1651\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[1;32m   1652\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[0;32m-> 1653\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/utils.py:1517\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1517\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1518\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[1;32m   1520\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   1521\u001b[0m     call_type\u001b[38;5;241m=\u001b[39mcall_type,\n\u001b[1;32m   1522\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/main.py:4159\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[0m\n\u001b[1;32m   4156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m   4157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   4158\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 4159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4162\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4165\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2346\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[1;32m   2345\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, litellm_response_headers)\n\u001b[0;32m-> 2346\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   2347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2348\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mLITELLM_EXCEPTION_TYPES:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_brochure_builder/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:365\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ExceptionCheckers\u001b[38;5;241m.\u001b[39mis_error_str_rate_limit(error_str):\n\u001b[1;32m    364\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\n\u001b[1;32m    366\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRateLimitError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    367\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    368\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m    369\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    370\u001b[0m     )\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ExceptionCheckers\u001b[38;5;241m.\u001b[39mis_error_str_context_window_exceeded(error_str):\n\u001b[1;32m    372\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: litellm.RateLimitError: RateLimitError: GroqException - {\"error\":{\"message\":\"Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kegx8yzce1fs824p93pwn7we` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199356, Requested 4586. Please try again in 28m22.944s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n"
     ]
    }
   ],
   "source": [
    "answer_question(\"Who went to Manchester University?\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51e3608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_brochure_builder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
